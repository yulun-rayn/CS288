{"cells":[{"cell_type":"markdown","metadata":{"id":"CeUOYktpXtOz"},"source":["# Project 3: Parsing and Transformers"]},{"cell_type":"markdown","metadata":{"id":"35m1Uv47jiNX"},"source":["This project covers constituency parsing and the Transformer neural network architecture.  First, you will implement a Transformer encoder, using a part-of-speech tagging task as an initial test of your implementation.  Then, you will implement a Transformer-based constituency parser.  Your parser will be trained to classify span labels, then you will implement the CKY algorithm to turn the predictions into trees.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"acPh_4GwYID0"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"c7cggO7mjZ5L"},"source":["The dependencies for this project include:\n","* `torch` for modeling and training\n","* `sentencepiece` for subword tokenization\n","* `nltk` for loading and working with parse tree data structures\n","* `svgling` for rendering parse trees in the browser"]},{"cell_type":"code","execution_count":103,"metadata":{"id":"4LV8KY6_unfe","trusted":true},"outputs":[],"source":["# Standard library imports\n","from copy import deepcopy\n","import json\n","import math\n","import random\n","import pdb\n","import os\n","\n","os.makedirs('./saves/', exist_ok=True)\n","\n","# Third party imports\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import sentencepiece\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","import tqdm.notebook\n","\n","import nltk\n","from nltk.corpus.reader.bracket_parse import BracketParseCorpusReader\n","\n","import svgling\n","svgling.disable_nltk_png()"]},{"cell_type":"markdown","metadata":{"id":"KNT_TURTwIlW"},"source":["Before proceeding, let's verify that we're connected to a GPU runtime and that `torch` can detect the GPU.\n","We'll define a variable `device` here to use throughout the code so that we can easily change to run on CPU for debugging."]},{"cell_type":"code","execution_count":104,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1623972368361,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"KYpIPtqtwVwh","outputId":"de34260e-cf8d-4b9f-9a82-cd281d4ae234","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found GPU\n","Using device: cuda\n"]}],"source":["if torch.cuda.is_available():\n","    print(\"Found GPU\")\n","    device = \"cuda\"\n","else:\n","    print(\"Did not find GPU\")\n","    device = \"cpu\"\n","print(\"Using device:\", device)"]},{"cell_type":"markdown","metadata":{"id":"ANK-5cMtYSyH"},"source":["## Data"]},{"cell_type":"markdown","metadata":{"id":"BZGmlRlB-2uo"},"source":["The code below downloads the standard Penn Treebank data splits for parsing: sections 2-21 are used for training, section 22 for validation, and section 23 for testing. For consistency, we'll use these same splits for part-of-speech tagging, but we should note that most academic papers use a different way of splitting up the Penn Treebank data for the part-of-speech tagging task."]},{"cell_type":"code","execution_count":107,"metadata":{"executionInfo":{"elapsed":508,"status":"ok","timestamp":1623972368864,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"ZMYj6yJKZ2j_","outputId":"058b98f2-92d6-496b-9632-11df18d79e7d","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  data/parsing-data.zip\n","  inflating: data/train              \n","  inflating: data/dev                \n","  inflating: data/test               \n","   creating: data/EVALB/\n","  inflating: data/EVALB/.DS_Store    \n","   creating: data/EVALB/bug/\n","  inflating: data/EVALB/bug/bug.gld  \n","  inflating: data/EVALB/bug/bug.rsl-new  \n","  inflating: data/EVALB/bug/bug.rsl-old  \n","  inflating: data/EVALB/bug/bug.tst  \n","  inflating: data/EVALB/COLLINS.prm  \n","  inflating: data/EVALB/evalb.c      \n","  inflating: data/EVALB/LICENSE      \n","  inflating: data/EVALB/Makefile     \n","  inflating: data/EVALB/new.prm      \n","  inflating: data/EVALB/nk.prm       \n","  inflating: data/EVALB/README       \n","   creating: data/EVALB/sample/\n","  inflating: data/EVALB/sample/sample.gld  \n","  inflating: data/EVALB/sample/sample.prm  \n","  inflating: data/EVALB/sample/sample.rsl  \n","  inflating: data/EVALB/sample/sample.tst  \n","  inflating: data/EVALB/tgrep_proc.prl  \n"]}],"source":["%%bash\n","if [ ! -e parsing-data.zip ]; then\n","    wget --quiet https://storage.googleapis.com/cs288-parsing-project/parsing-data.zip -P data/\n","fi\n","rm -rf data/train data/dev data/test data/EVALB/\n","unzip data/parsing-data.zip -d data/"]},{"cell_type":"markdown","metadata":{"id":"FBZaWRRf_n_i"},"source":["Let's take a look at the format of the data:"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1623972368865,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"sNUNK7Bg_qgx","outputId":"762b7172-36e8-4bc0-da14-e8f9200e52e8","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(TOP (S (PP (IN In) (NP (NP (DT an) (NNP Oct.) (CD 19) (NN review)) (PP (IN of) (NP (`` ``) (NP (DT The) (NN Misanthrope)) ('' '') (PP (IN at) (NP (NP (NNP Chicago) (POS 's)) (NNP Goodman) (NNP Theatre))))) (PRN (-LRB- -LRB-) (`` ``) (S (NP (VBN Revitalized) (NNS Classics)) (VP (VBP Take) (NP (DT the) (NN Stage)) (PP (IN in) (NP (NNP Windy) (NNP City))))) (, ,) ('' '') (NP (NN Leisure) (CC &) (NNS Arts)) (-RRB- -RRB-)))) (, ,) (NP (NP (NP (DT the) (NN role)) (PP (IN of) (NP (NNP Celimene)))) (, ,) (VP (VBN played) (PP (IN by) (NP (NNP Kim) (NNP Cattrall)))) (, ,)) (VP (VBD was) (VP (ADVP (RB mistakenly)) (VBN attributed) (PP (TO to) (NP (NNP Christina) (NNP Haag))))) (. .)))\n","(TOP (S (NP (NNP Ms.) (NNP Haag)) (VP (VBZ plays) (NP (NNP Elianti))) (. .)))\n"]}],"source":["!head -n 2 data/train"]},{"cell_type":"markdown","metadata":{"id":"i6-ahhuG_rIN"},"source":["The files include one tree per line. We'll use the `BracketParseCorpusReader` from `nltk` to load the data."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"eGzu6IjkBW74","trusted":true},"outputs":[],"source":["READER = BracketParseCorpusReader('data/', ['train', 'dev', 'test'])"]},{"cell_type":"markdown","metadata":{"id":"k1aBbR3snOjV"},"source":["## Vocabulary"]},{"cell_type":"markdown","metadata":{"id":"asT2gaZ7_48T"},"source":["We first extract the sentences alone from the data and construct a subword vocabulary, much like in Project 2. We use a subword vocabulary because it allows us to largely avoid the issue of having unknown words at test time."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1623972368866,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"WSyRQHA4CIWO","outputId":"034512cb-d8ec-494e-8d2b-591ce1497d35","trusted":true},"outputs":[{"data":{"text/plain":["['Ms.', 'Haag', 'plays', 'Elianti', '.']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["READER.sents('train')[1]"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Yasw_R3hB_13","trusted":true},"outputs":[],"source":["with open('data/sentences.txt', 'w') as f:\n","    for sent in READER.sents('train'):\n","        f.write(' '.join(sent) + '\\n')"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":340,"status":"ok","timestamp":1623972371683,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"uxfmTYt_B8Cm","outputId":"d76454c2-cffd-4daa-f843-e9bf5857fdad","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["In an Oct. 19 review of `` The Misanthrope '' at Chicago 's Goodman Theatre -LRB- `` Revitalized Classics Take the Stage in Windy City , '' Leisure & Arts -RRB- , the role of Celimene , played by Kim Cattrall , was mistakenly attributed to Christina Haag .\n","Ms. Haag plays Elianti .\n"]}],"source":["!head -n 2 data/sentences.txt"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"8xSUaso9vo1V","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["sentencepiece_trainer.cc(177) LOG(INFO) Running command: --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3 --input=data/sentences.txt --vocab_size=16000 --model_prefix=ptb\n","sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n","trainer_spec {\n","  input: data/sentences.txt\n","  input_format: \n","  model_prefix: ptb\n","  model_type: UNIGRAM\n","  vocab_size: 16000\n","  self_test_sample_size: 0\n","  character_coverage: 0.9995\n","  input_sentence_size: 0\n","  shuffle_input_sentence: 1\n","  seed_sentencepiece_size: 1000000\n","  shrinking_factor: 0.75\n","  max_sentence_length: 4192\n","  num_threads: 16\n","  num_sub_iterations: 2\n","  max_sentencepiece_length: 16\n","  split_by_unicode_script: 1\n","  split_by_number: 1\n","  split_by_whitespace: 1\n","  split_digits: 0\n","  treat_whitespace_as_suffix: 0\n","  allow_whitespace_only_pieces: 0\n","  required_chars: \n","  byte_fallback: 0\n","  vocabulary_output_piece_score: 1\n","  train_extremely_large_corpus: 0\n","  hard_vocab_limit: 1\n","  use_all_vocab: 0\n","  unk_id: 3\n","  bos_id: 1\n","  eos_id: 2\n","  pad_id: 0\n","  unk_piece: <unk>\n","  bos_piece: <s>\n","  eos_piece: </s>\n","  pad_piece: <pad>\n","  unk_surface:  ‚Åá \n","  enable_differential_privacy: 0\n","  differential_privacy_noise_level: 0\n","  differential_privacy_clipping_threshold: 0\n","}\n","normalizer_spec {\n","  name: nmt_nfkc\n","  add_dummy_prefix: 1\n","  remove_extra_whitespaces: 1\n","  escape_whitespaces: 1\n","  normalization_rule_tsv: \n","}\n","denormalizer_spec {}\n","trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n","trainer_interface.cc(181) LOG(INFO) Loading corpus: data/sentences.txt\n","trainer_interface.cc(406) LOG(INFO) Loaded all 39832 sentences\n","trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <pad>\n","trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n","trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n","trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n","trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n","trainer_interface.cc(536) LOG(INFO) all chars count=5190396\n","trainer_interface.cc(547) LOG(INFO) Done: 99.9531% characters are covered.\n","trainer_interface.cc(557) LOG(INFO) Alphabet size=71\n","trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999531\n","trainer_interface.cc(590) LOG(INFO) Done! preprocessed 39832 sentences.\n","unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n","unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n","unigram_model_trainer.cc(201) LOG(INFO) Initialized 69388 seed sentencepieces\n","trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 39832\n","trainer_interface.cc(607) LOG(INFO) Done! 44379\n","unigram_model_trainer.cc(491) LOG(INFO) Using 44379 sentences for EM training\n","unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=30595 obj=11.1377 num_tokens=92810 num_tokens/piece=3.0335\n","unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=24750 obj=9.16256 num_tokens=93832 num_tokens/piece=3.79119\n","unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=18550 obj=9.09699 num_tokens=98603 num_tokens/piece=5.31553\n","unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=18542 obj=9.06246 num_tokens=98740 num_tokens/piece=5.32521\n","unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=17599 obj=9.06586 num_tokens=100108 num_tokens/piece=5.68828\n","unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=17596 obj=9.06102 num_tokens=100274 num_tokens/piece=5.69868\n","trainer_interface.cc(685) LOG(INFO) Saving model: ptb.model\n","trainer_interface.cc(697) LOG(INFO) Saving vocabs: ptb.vocab\n"]}],"source":["args = {\n","    \"pad_id\": 0,\n","    \"bos_id\": 1,\n","    \"eos_id\": 2,\n","    \"unk_id\": 3,\n","    \"input\": \"data/sentences.txt\",\n","    \"vocab_size\": 16000,\n","    \"model_prefix\": \"ptb\",\n","}\n","combined_args = \" \".join(\n","    \"--{}={}\".format(key, value) for key, value in args.items())\n","sentencepiece.SentencePieceTrainer.Train(combined_args)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":41,"status":"ok","timestamp":1623972376247,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"Xj-OHYlppX4N","outputId":"19d58656-d417-483a-f12a-4ae3491b6af1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<pad>\t0\n","<s>\t0\n","</s>\t0\n","<unk>\t0\n","s\t-2.85529\n","‚ñÅ,\t-3.27864\n","‚ñÅthe\t-3.386\n","‚ñÅ.\t-3.51108\n","‚ñÅ\t-3.70671\n","‚ñÅto\t-4.02158\n"]}],"source":["!head -n 10 ptb.vocab"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1623972376248,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"SJYzMQxfvrr0","outputId":"41414096-5ed0-46e7-e40e-b83a00171877","trusted":true},"outputs":[{"data":{"text/plain":["True"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["VOCAB = sentencepiece.SentencePieceProcessor()\n","VOCAB.Load(\"ptb.model\")"]},{"cell_type":"markdown","metadata":{"id":"qsxRb6TOVHn5"},"source":["We define some constants here for special tokens that you may find useful in the following sections."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"xheKi30BVVJC","trusted":true},"outputs":[],"source":["PAD_ID = VOCAB.PieceToId(\"<pad>\")\n","BOS_ID = VOCAB.PieceToId(\"<s>\")\n","EOS_ID = VOCAB.PieceToId(\"</s>\")\n","UNK_ID = VOCAB.PieceToId(\"<unk>\")"]},{"cell_type":"markdown","metadata":{"id":"-uzrJDCU1Tei"},"source":["## Part-of-Speech Tagging: Task Setup"]},{"cell_type":"markdown","metadata":{"id":"1P3NgM0cBHnD"},"source":["We will now begin preparing for the part-of-speech tagging task.  In this task, we will label each word token in a sentence or corpus with a part-of-speech tag.  Note the difference between operating over word tokens in a corpus and Project 0 where we operated over unique word types.  In this setup, we may see each word type many times, and we also take the context where it appears into account.\n","\n","For this task we provide you with all of the data processing code, while you will implement the model using the Transformer architecture. However, please take note of the input pipeline here because you will need to implement your own data processing for the second half of the project.\n","\n","Although we would like to use a subword vocabulary to better handle rare words, the part-of-speech and parsing tasks are defined in terms of words, not subwords.  After encoding a sentence at the subword level with an encoder (LSTM or Transformer), we will then move to the word level by selecting a single representation per word.  In the `encode_sentence` function below, we will create a boolean mask to select from the last subword of every word.  Tagging decisions will be made based on the vector associated with this last subword. (You may modify this to, for example, use the first word piece instead, though that seems to perform slightly worse at least for the baseline model.)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"tvlk8axRvGiB","trusted":true},"outputs":[],"source":["def encode_sentence(sent):\n","    \"\"\"Prepares a sentence for input to the model, including subword tokenization.\n","\n","    Args:\n","        sent: a list of words (each word is a string)\n","    Returns:\n","        A tuple (ids, is_word_end).\n","            ids: a list of token ids in the subword vocabulary\n","            is_word_end: a list with elements of type bool, where True indicates that\n","                        the word piece at that position is the last within its word.\n","    \"\"\"\n","    ids = []\n","    is_word_end = []\n","    for word in sent:\n","        word_ids = VOCAB.EncodeAsIds(word)\n","        ids.extend(word_ids)\n","        is_word_end.extend([False] * (len(word_ids) - 1) + [True])\n","    return ids, is_word_end"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1623972376249,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"5ww4AEIRvA6P","outputId":"b819ed44-8306-470e-d20b-879d58c1a5d9","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary size: 16000\n","\n","['In', 'an', 'Oct.', '19', 'review', 'of', '``', 'The', 'Misanthrope', \"''\", 'at', 'Chicago', \"'s\", 'Goodman', 'Theatre', '-LRB-', '``', 'Revitalized', 'Classics', 'Take', 'the', 'Stage', 'in', 'Windy', 'City', ',', \"''\", 'Leisure', '&', 'Arts', '-RRB-', ',', 'the', 'role', 'of', 'Celimene', ',', 'played', 'by', 'Kim', 'Cattrall', ',', 'was', 'mistakenly', 'attributed', 'to', 'Christina', 'Haag', '.']\n","['‚ñÅIn', '‚ñÅan', '‚ñÅOct', '.', '‚ñÅ19', '‚ñÅreview', '‚ñÅof', '‚ñÅ``', '‚ñÅThe', '‚ñÅMis', 'anthrop', 'e', \"‚ñÅ''\", '‚ñÅat', '‚ñÅChicago', \"‚ñÅ'\", 's', '‚ñÅGood', 'man', '‚ñÅThe', 'at', 're', '‚ñÅ-', 'L', 'RB', '-', '‚ñÅ``', '‚ñÅRev', 'ital', 'ized', '‚ñÅClassic', 's', '‚ñÅTake', '‚ñÅthe', '‚ñÅSt', 'age', '‚ñÅin', '‚ñÅWind', 'y', '‚ñÅCity', '‚ñÅ,', \"‚ñÅ''\", '‚ñÅL', 'eisure', '‚ñÅ', '&', '‚ñÅArt', 's', '‚ñÅ-', 'R', 'RB', '-', '‚ñÅ,', '‚ñÅthe', '‚ñÅrole', '‚ñÅof', '‚ñÅCel', 'imene', '‚ñÅ,', '‚ñÅplay', 'ed', '‚ñÅby', '‚ñÅKim', '‚ñÅCa', 't', 't', 'rall', '‚ñÅ,', '‚ñÅwas', '‚ñÅmistaken', 'ly', '‚ñÅattribute', 'd', '‚ñÅto', '‚ñÅChristin', 'a', '‚ñÅHaag', '‚ñÅ.']\n","[True, True, False, True, True, True, True, True, True, False, False, True, True, True, True, False, True, False, True, False, False, True, False, False, False, True, True, False, False, True, False, True, True, True, False, True, True, False, True, True, True, True, False, True, False, True, False, True, False, False, False, True, True, True, True, True, False, True, True, False, True, True, True, False, False, False, True, True, True, False, True, False, True, True, False, True, True, True]\n","In an Oct. 19 review of `` The Misanthrope '' at Chicago 's Goodman Theatre -LRB- `` Revitalized Classics Take the Stage in Windy City , '' Leisure & Arts -RRB- , the role of Celimene , played by Kim Cattrall , was mistakenly attributed to Christina Haag .\n","[67, 45, 279, 14, 648, 1475, 10, 28, 25, 8232, 13326, 50, 30, 40, 661, 20, 4, 2060, 172, 25, 688, 180, 59, 92, 61, 19, 28, 2941, 1346, 1344, 7954, 4, 6381, 6, 1293, 1111, 16, 8054, 21, 1017, 5, 30, 516, 13789, 8, 103, 3050, 4, 59, 90, 61, 19, 5, 6, 1073, 10, 8301, 11897, 5, 711, 12, 36, 5354, 4662, 17, 17, 14568, 5, 41, 8602, 22, 2123, 13, 9, 14798, 132, 9362, 7]\n","In an Oct. 19 review of `` The Misanthrope '' at Chicago 's Goodman Theatre -LRB- `` Revitalized Classics Take the Stage in Windy City , '' Leisure & Arts -RRB- , the role of Celimene , played by Kim Cattrall , was mistakenly attributed to Christina Haag .\n","\n","['Ms.', 'Haag', 'plays', 'Elianti', '.']\n","['‚ñÅM', 's', '.', '‚ñÅHaag', '‚ñÅplay', 's', '‚ñÅEli', 'anti', '‚ñÅ.']\n","[False, False, True, True, False, True, False, True, True]\n","Ms. Haag plays Elianti .\n","[126, 4, 14, 9362, 711, 4, 4386, 8675, 7]\n","Ms. Haag plays Elianti .\n","\n"]}],"source":["print(\"Vocabulary size:\", VOCAB.GetPieceSize())\n","print()\n","\n","for sent in READER.sents('train')[:2]:\n","    indices, is_word_end = encode_sentence(sent)\n","    pieces = [VOCAB.IdToPiece(index) for index in indices]\n","    print(sent)\n","    print(pieces)\n","    print(is_word_end)\n","    print(VOCAB.DecodePieces(pieces))\n","    print(indices)\n","    print(VOCAB.DecodeIds(indices))\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"nZZGwaM4qWK6"},"source":["Now we turn our attention to the desired output from the model, namely a sequence of part of speech tags. The `READER` object from NLTK has a method for returing word-and-tag tuples read from the data."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1623972376250,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"tmO_829vG9RO","outputId":"55ebbdd5-ea04-4cd0-fde8-4b260fcc63e5","trusted":true},"outputs":[{"data":{"text/plain":["[('Ms.', 'NNP'),\n"," ('Haag', 'NNP'),\n"," ('plays', 'VBZ'),\n"," ('Elianti', 'NNP'),\n"," ('.', '.')]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["READER.tagged_sents('train')[1]"]},{"cell_type":"markdown","metadata":{"id":"gY26VRXtBR5J"},"source":["Running the cell below will print a definition and some examples for each part-of-speech class."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1623972376251,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"-iDS1lcXvQaN","outputId":"bc254ec5-8ee4-4388-edc5-a3401c154f73","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["$: dollar\n","    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n","'': closing quotation mark\n","    ' ''\n","(: opening parenthesis\n","    ( [ {\n","): closing parenthesis\n","    ) ] }\n",",: comma\n","    ,\n","--: dash\n","    --\n",".: sentence terminator\n","    . ! ?\n",":: colon or ellipsis\n","    : ; ...\n","CC: conjunction, coordinating\n","    & 'n and both but either et for less minus neither nor or plus so\n","    therefore times v. versus vs. whether yet\n","CD: numeral, cardinal\n","    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n","    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n","    fifteen 271,124 dozen quintillion DM2,000 ...\n","DT: determiner\n","    all an another any both del each either every half la many much nary\n","    neither no some such that the them these this those\n","EX: existential there\n","    there\n","FW: foreign word\n","    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n","    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n","    terram fiche oui corporis ...\n","IN: preposition or conjunction, subordinating\n","    astride among uppon whether out inside pro despite on by throughout\n","    below within for towards near behind atop around if like until below\n","    next into if beside ...\n","JJ: adjective or numeral, ordinal\n","    third ill-mannered pre-war regrettable oiled calamitous first separable\n","    ectoplasmic battery-powered participatory fourth still-to-be-named\n","    multilingual multi-disciplinary ...\n","JJR: adjective, comparative\n","    bleaker braver breezier briefer brighter brisker broader bumper busier\n","    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n","    cozier creamier crunchier cuter ...\n","JJS: adjective, superlative\n","    calmest cheapest choicest classiest cleanest clearest closest commonest\n","    corniest costliest crassest creepiest crudest cutest darkest deadliest\n","    dearest deepest densest dinkiest ...\n","LS: list item marker\n","    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n","    SP-44007 Second Third Three Two * a b c d first five four one six three\n","    two\n","MD: modal auxiliary\n","    can cannot could couldn't dare may might must need ought shall should\n","    shouldn't will would\n","NN: noun, common, singular or mass\n","    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n","    investment slide humour falloff slick wind hyena override subhumanity\n","    machinist ...\n","NNP: noun, proper, singular\n","    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n","    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n","    Shannon A.K.C. Meltex Liverpool ...\n","NNPS: noun, proper, plural\n","    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n","    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n","    Apache Apaches Apocrypha ...\n","NNS: noun, common, plural\n","    undergraduates scotches bric-a-brac products bodyguards facets coasts\n","    divestitures storehouses designs clubs fragrances averages\n","    subjectivists apprehensions muses factory-jobs ...\n","PDT: pre-determiner\n","    all both half many quite such sure this\n","POS: genitive marker\n","    ' 's\n","PRP: pronoun, personal\n","    hers herself him himself hisself it itself me myself one oneself ours\n","    ourselves ownself self she thee theirs them themselves they thou thy us\n","PRP$: pronoun, possessive\n","    her his mine my our ours their thy your\n","RB: adverb\n","    occasionally unabatingly maddeningly adventurously professedly\n","    stirringly prominently technologically magisterially predominately\n","    swiftly fiscally pitilessly ...\n","RBR: adverb, comparative\n","    further gloomier grander graver greater grimmer harder harsher\n","    healthier heavier higher however larger later leaner lengthier less-\n","    perfectly lesser lonelier longer louder lower more ...\n","RBS: adverb, superlative\n","    best biggest bluntest earliest farthest first furthest hardest\n","    heartiest highest largest least less most nearest second tightest worst\n","RP: particle\n","    aboard about across along apart around aside at away back before behind\n","    by crop down ever fast for forth from go high i.e. in into just later\n","    low more off on open out over per pie raising start teeth that through\n","    under unto up up-pp upon whole with you\n","SYM: symbol\n","    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n","TO: \"to\" as preposition or infinitive marker\n","    to\n","UH: interjection\n","    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n","    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n","    man baby diddle hush sonuvabitch ...\n","VB: verb, base form\n","    ask assemble assess assign assume atone attention avoid bake balkanize\n","    bank begin behold believe bend benefit bevel beware bless boil bomb\n","    boost brace break bring broil brush build ...\n","VBD: verb, past tense\n","    dipped pleaded swiped regummed soaked tidied convened halted registered\n","    cushioned exacted snubbed strode aimed adopted belied figgered\n","    speculated wore appreciated contemplated ...\n","VBG: verb, present participle or gerund\n","    telegraphing stirring focusing angering judging stalling lactating\n","    hankerin' alleging veering capping approaching traveling besieging\n","    encrypting interrupting erasing wincing ...\n","VBN: verb, past participle\n","    multihulled dilapidated aerosolized chaired languished panelized used\n","    experimented flourished imitated reunifed factored condensed sheared\n","    unsettled primed dubbed desired ...\n","VBP: verb, present tense, not 3rd person singular\n","    predominate wrap resort sue twist spill cure lengthen brush terminate\n","    appear tend stray glisten obtain comprise detest tease attract\n","    emphasize mold postpone sever return wag ...\n","VBZ: verb, present tense, 3rd person singular\n","    bases reconstructs marks mixes displeases seals carps weaves snatches\n","    slumps stretches authorizes smolders pictures emerges stockpiles\n","    seduces fizzes uses bolsters slaps speaks pleads ...\n","WDT: WH-determiner\n","    that what whatever which whichever\n","WP: WH-pronoun\n","    that what whatever whatsoever which who whom whosoever\n","WP$: WH-pronoun, possessive\n","    whose\n","WRB: Wh-adverb\n","    how however whence whenever where whereby whereever wherein whereof why\n","``: opening quotation mark\n","    ` ``\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package tagsets to /home/ubuntu/nltk_data...\n","[nltk_data]   Package tagsets is already up-to-date!\n"]}],"source":["nltk.download('tagsets')\n","nltk.help.upenn_tagset()"]},{"cell_type":"markdown","metadata":{"id":"veJzgeJDBdw1"},"source":["We construct a part of speech tag vocabulary by iterating over all tags in the training data. Note that opening parentheses `(` are escaped as `-LRB-` in the data format (and similarly `)` is escaped as `-RRB-`)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":2544,"status":"ok","timestamp":1623972379083,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"u_QW5pk2G7-W","outputId":"0b0e0c95-354e-4016-e673-880797bad2d1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['#', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"]}],"source":["def get_pos_vocab():\n","    all_pos = set()\n","    for sent in READER.tagged_sents('train'):\n","        for word, pos in sent:\n","            all_pos.add(pos)\n","    return sorted(all_pos)\n","\n","PARTS_OF_SPEECH = get_pos_vocab()\n","print(PARTS_OF_SPEECH)"]},{"cell_type":"markdown","metadata":{"id":"ifj0Isu3B5Hj"},"source":["The `POSTaggingDataset` object defined below is a PyTorch Dataset object for this task.\n","\n","Each example in the dataset is a feature dictionary, consisting of word piece `ids`, and corresponding label ids (`labels`). We associate a word's label with the last subword. Any remaining subwords, as well as special tokens like the start token or padding token, will have a label of -1 assigned to them. This will signal that we shouldn't compute a loss for that label.\n","\n","We also define a `collate` function that takes care of padding when examples are batched together. "]},{"cell_type":"code","execution_count":18,"metadata":{"id":"dT4iBQ1eBoSJ","trusted":true},"outputs":[],"source":["class POSTaggingDataset(torch.utils.data.Dataset):\n","    def __init__(self, split):\n","        assert split in ('train', 'dev', 'test')\n","        self.sents = READER.tagged_sents(split)\n","        if split == 'train':\n","            # To speed up training, we only train on short sentences.\n","            self.sents = [sent for sent in self.sents if len(sent) <= 40]\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","    def __getitem__(self, index):\n","        sent = self.sents[index]\n","        ids, is_word_end = encode_sentence([word for word, pos in sent])\n","        ids = [BOS_ID] + ids + [EOS_ID]\n","        is_word_end = [False] + is_word_end + [False]\n","        ids = torch.tensor(ids)\n","        is_word_end = torch.tensor(is_word_end)\n","        labels = torch.full_like(ids, -1)\n","        labels[is_word_end] = torch.tensor(\n","            [PARTS_OF_SPEECH.index(pos) for word, pos in sent])\n","        return {'ids': ids, 'labels': labels}\n","\n","    @staticmethod\n","    def collate(batch):\n","        ids = pad_sequence(\n","            [item['ids'] for item in batch],\n","            batch_first=True, padding_value=PAD_ID)\n","        labels = pad_sequence(\n","            [item['labels'] for item in batch],\n","            batch_first=True, padding_value=-1)\n","        return {'ids': ids.to(device), 'labels': labels.to(device)}"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":2905,"status":"ok","timestamp":1623972381975,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"gt5E2Pr_skQL","outputId":"97679199-3118-40f0-bdc7-27158ad017e1","trusted":true},"outputs":[{"data":{"text/plain":["{'ids': tensor([   1,  126,    4,   14, 9362,  711,    4, 4386, 8675,    7,    2]),\n"," 'labels': tensor([-1, -1, -1, 20, 20, -1, 39, -1, 20,  6, -1])}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["dataset_for_inspection = POSTaggingDataset('train')\n","datum = dataset_for_inspection[0]\n","datum"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1623972381975,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"73v2WPBAGDFN","outputId":"0cf8dfee-8d8c-4a6a-aabf-c865fe1d2013","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" 0 -     <s>\n"," 1 -     ‚ñÅM\n"," 2 -     s\n"," 3 NNP   .\n"," 4 NNP   ‚ñÅHaag\n"," 5 -     ‚ñÅplay\n"," 6 VBZ   s\n"," 7 -     ‚ñÅEli\n"," 8 NNP   anti\n"," 9 .     ‚ñÅ.\n","10 -     </s>\n"]}],"source":["for i, (piece_id, label) in enumerate(zip(datum['ids'].tolist(),\n","                                          datum['labels'].tolist())):\n","  print('{:2d} {: <5} {}'.format(\n","      i, \"-\" if label == -1 else PARTS_OF_SPEECH[label],\n","      VOCAB.IdToPiece(piece_id)))"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":10606,"status":"ok","timestamp":1623972392577,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"drIp8PNRJ_hP","outputId":"cbace93e-a94a-4418-8834-b418dbc89c90","trusted":true},"outputs":[{"data":{"text/plain":["{'ids': tensor([[    1,  1951,  1138,     5,    70,  1095,    10,  8711, 10502,  1389,\n","           4750,    19,  6204,    18,    29,   215,    12,    28,  5737,   323,\n","             30,    36,  5016,     5,   265,    97,     6,   477,  1095,    10,\n","            551, 10663,  1321,  1675,  1192,    40,  2460,    29,  3615,    12,\n","             28,  1910,  7135,    18,     7,    30,     2],\n","         [    1,    62, 10287,    32,    13,    48,  1022,   453,    10,   701,\n","             34,    10,    62, 10287,  6351,  4194,    85,    14,    29,   155,\n","             12,     9,   446,    16,    11,   236,   348,    10,    66,     8,\n","             27,  1479,    38,     5,    63,    51,    26,   406,     9,   277,\n","            158,    62, 10287,   290,     7,     2,     0]], device='cuda:0'),\n"," 'labels': tensor([[-1, 13, 19,  3, 10, 19, 13, -1, 14, 14, -1, -1, -1, 19, 39, -1, 37, 44,\n","          14, 19,  2, 13, 22,  3, -1, 13, 10, 14, 19, 13, -1, 14, -1, -1, 19, 13,\n","          20, 39, -1, 37, 44, 19, -1, 19,  6,  2, -1],\n","         [-1, -1, 20, -1, 35, 26, 14, 19, 13,  9, 19, 13, -1, 20, -1, 20, -1, 20,\n","          39, -1, 37, 32, 34, 13, 10, 14, 19, 13, 27, -1,  1,  9,  9,  3, 40, 18,\n","          34, 37, 32, 34, 30, -1, 20, 19,  6, -1, -1]], device='cuda:0')}"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["data_loader_for_inspection = torch.utils.data.DataLoader(\n","    dataset_for_inspection, batch_size=2, shuffle=True,\n","    collate_fn=dataset_for_inspection.collate)\n","next(iter(data_loader_for_inspection))"]},{"cell_type":"markdown","metadata":{"id":"KiSPP3tdyaid"},"source":["## Training Loop and Baseline POS Tagging Model"]},{"cell_type":"markdown","metadata":{"id":"gGIIy0230Zdc"},"source":["Now it's time to build a model. At a high level, the model will encode the sentence using a Transformer architecture, then project to a softmax over the vocabulary at each word position.  We've implemented the overall model framework already, including computing the softmax cross-entropy loss for training the tagger."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"Dz3pUko2L8Fj","trusted":true},"outputs":[],"source":["class POSTaggingModel(nn.Module):\n","    def encode(self, batch):\n","        # you will override this function in a subclass below\n","        raise NotImplementedError()\n","\n","    def compute_loss(self, batch):\n","        logits = self.encode(batch)\n","        logits = logits.reshape((-1, logits.shape[-1]))\n","        labels = batch['labels'].reshape((-1,))\n","        res = F.cross_entropy(logits, labels, ignore_index=-1, reduction='mean')\n","        return res\n","\n","    def get_validation_metric(self, batch_size=8):\n","        dataset = POSTaggingDataset('dev')\n","        data_loader = torch.utils.data.DataLoader(\n","            dataset, batch_size=batch_size, collate_fn=dataset.collate)\n","        self.eval()\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for batch in data_loader:\n","                mask = (batch['labels'] != -1)\n","                predicted_labels = self.encode(batch).argmax(-1)\n","                predicted_labels = predicted_labels[mask]\n","                gold_labels = batch['labels'][mask]\n","                correct += (predicted_labels == gold_labels).sum().item()\n","                total += gold_labels.shape[0]\n","        return correct / total"]},{"cell_type":"markdown","metadata":{"id":"jx9jJQb58nUY"},"source":["We define the following functions for training.  This code will run as provided, but you are welcome to modify the training loop to adjust the optimizer settings, modify the learning rate schedule, etc.\n","\n","When training transformers, it has been found that early training can be unstable unless the learning rate starts out very low.  To alleviate this instability, we'll use a schedule where the learning rate is increased linearly from 0 to its maximum value during a warm-up phase, and is then decayed as training progresses.\n","\n","You may change the duration of the warmup phase or disable it entirely, but bear in mind that warmup is generally seen as a key ingredient for stably training Transformer models. "]},{"cell_type":"code","execution_count":23,"metadata":{"id":"VKc1pwaZzcUN","trusted":true},"outputs":[],"source":["def train(model, num_epochs, batch_size, model_file,\n","          learning_rate=8e-4, dataset_cls=POSTaggingDataset):\n","    \"\"\"Train the model and save its best checkpoint.\n","\n","    Model performance across epochs is evaluated on the validation set. The best\n","    checkpoint obtained during training will be stored on disk and loaded back\n","    into the model at the end of training.\n","    \"\"\"\n","    dataset = dataset_cls('train')\n","    data_loader = torch.utils.data.DataLoader(\n","        dataset, batch_size=batch_size, shuffle=True, collate_fn=dataset.collate)\n","    optimizer = torch.optim.Adam(\n","        model.parameters(),\n","        lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n","    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","        optimizer,\n","        learning_rate,\n","        epochs=num_epochs,\n","        steps_per_epoch=len(data_loader),\n","        pct_start=0.02,  # Warm up for 2% of the total training time\n","    )\n","    best_metric = 0.0\n","    for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n","        with tqdm.notebook.tqdm(\n","            data_loader,\n","            desc=\"epoch {}\".format(epoch + 1),\n","            unit=\"batch\",\n","            total=len(data_loader)\n","        ) as batch_iterator:\n","            model.train()\n","            total_loss = 0.0\n","            for i, batch in enumerate(batch_iterator, start=1):\n","                optimizer.zero_grad()\n","                loss = model.compute_loss(batch)\n","                total_loss += loss.item()\n","                loss.backward()\n","                optimizer.step()\n","                scheduler.step()\n","                batch_iterator.set_postfix(mean_loss=total_loss / i)\n","            validation_metric = model.get_validation_metric()\n","            batch_iterator.set_postfix(\n","                mean_loss=total_loss / i,\n","                validation_metric=validation_metric)\n","            if validation_metric > best_metric:\n","                print(\n","                    \"Obtained a new best validation metric of {:.3f}, saving model \"\n","                    \"checkpoint to {}...\".format(validation_metric, model_file))\n","                torch.save(model.state_dict(), model_file)\n","                best_metric = validation_metric\n","    print(\"Reloading best model checkpoint from {}...\".format(model_file))\n","    model.load_state_dict(torch.load(model_file))"]},{"cell_type":"markdown","metadata":{"id":"GZYFBChe8-2F"},"source":["We can now train a very simple baseline model that learns a single parameter per part of speech tag.\n","\n","A classifier where each word is assigned its most-frequent tag from the training data (and unknown words are treated as nouns) has 92.2% validation accuracy with our current splits. However, with our subword vocabulary (and taking the last subword as representative of each word), the accuracy is instead 87.6%. The `BaselineModel` should achieve roughly this accuracy."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"pdKjXH-a2HyF","trusted":true},"outputs":[],"source":["class BaselineModel(POSTaggingModel):\n","    def __init__(self):\n","        super().__init__()\n","        self.lookup = nn.Embedding(VOCAB.GetPieceSize(), len(PARTS_OF_SPEECH))\n","\n","    def encode(self, batch):\n","        ids = batch['ids']\n","        return self.lookup(ids)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"thNLsZ04Z0A-","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ca5a6fa30f04d80a931c122bd7ee1a8","version_major":2,"version_minor":0},"text/plain":["training:   0%|          | 0/5 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15f0e2f973fb4d86a26c36d976928e2b","version_major":2,"version_minor":0},"text/plain":["epoch 1:   0%|          | 0/575 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Obtained a new best validation metric of 0.870, saving model checkpoint to ./saves/baseline_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2212f27d34fd41e68006eeff7adbf84e","version_major":2,"version_minor":0},"text/plain":["epoch 2:   0%|          | 0/575 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Obtained a new best validation metric of 0.875, saving model checkpoint to ./saves/baseline_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c39c4ebadd704cccaae26b357ee2898c","version_major":2,"version_minor":0},"text/plain":["epoch 3:   0%|          | 0/575 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Obtained a new best validation metric of 0.876, saving model checkpoint to ./saves/baseline_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"964d06928aae4691849e0202c6d5a48f","version_major":2,"version_minor":0},"text/plain":["epoch 4:   0%|          | 0/575 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f317d35f59ac498c8467c5e34b9ea6fb","version_major":2,"version_minor":0},"text/plain":["epoch 5:   0%|          | 0/575 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Reloading best model checkpoint from ./saves/baseline_model.pt...\n"]}],"source":["baseline_model = BaselineModel().to(device)\n","train(baseline_model, num_epochs=5, batch_size=64,\n","      model_file=\"./saves/baseline_model.pt\", learning_rate=0.1)"]},{"cell_type":"markdown","metadata":{"id":"4MEVY7vOKVbp"},"source":["Having trained the model, we can examine its predictions on an example from the validation set."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Gh7Kx_R8VH0Q","trusted":true},"outputs":[],"source":["def predict_tags(tagging_model, split, limit=None):\n","    assert split in ('dev', 'test')\n","    sents = READER.sents(split)\n","    dataset = POSTaggingDataset(split)\n","    data_loader = torch.utils.data.DataLoader(\n","        dataset, batch_size=8, shuffle=False, collate_fn=dataset.collate)\n","    tagging_model.eval()\n","    pred_tagged_sents = []\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            mask = (batch['labels'] != -1)\n","            predicted_labels = tagging_model.encode(batch).argmax(-1)\n","            for i in range(batch['ids'].shape[0]):\n","                example_predicted_tags = [\n","                    PARTS_OF_SPEECH[label] for label in predicted_labels[i][mask[i]]]\n","                sent = sents[len(pred_tagged_sents)]\n","                assert len(sent) == len(example_predicted_tags)\n","                pred_tagged_sents.append(list(zip(sent, example_predicted_tags)))\n","                if limit is not None and len(pred_tagged_sents) >= limit:\n","                    return pred_tagged_sents\n","    return pred_tagged_sents"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":138,"status":"ok","timestamp":1623972392871,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"_ZjD3vvyWg0s","outputId":"d97b3dd0-6304-45ab-eb69-71d28e8857b4","trusted":true},"outputs":[{"data":{"text/plain":["[[('Influential', 'JJ'),\n","  ('members', 'NNS'),\n","  ('of', 'IN'),\n","  ('the', 'DT'),\n","  ('House', 'NNP'),\n","  ('Ways', 'NNS'),\n","  ('and', 'CC'),\n","  ('Means', 'NNS'),\n","  ('Committee', 'NNP'),\n","  ('introduced', 'VBD'),\n","  ('legislation', 'NN'),\n","  ('that', 'IN'),\n","  ('would', 'MD'),\n","  ('restrict', 'VB'),\n","  ('how', 'WRB'),\n","  ('the', 'DT'),\n","  ('new', 'JJ'),\n","  ('savings-and-loan', 'NNP'),\n","  ('bailout', 'NN'),\n","  ('agency', 'NN'),\n","  ('can', 'MD'),\n","  ('raise', 'VB'),\n","  ('capital', 'NN'),\n","  (',', ','),\n","  ('creating', 'VBG'),\n","  ('another', 'DT'),\n","  ('potential', 'JJ'),\n","  ('obstacle', 'NN'),\n","  ('to', 'TO'),\n","  ('the', 'DT'),\n","  ('government', 'NN'),\n","  (\"'s\", 'NNS'),\n","  ('sale', 'NN'),\n","  ('of', 'IN'),\n","  ('sick', 'JJ'),\n","  ('thrifts', 'NNS'),\n","  ('.', '.')]]"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["predict_tags(baseline_model, 'dev', limit=1)"]},{"cell_type":"markdown","metadata":{"id":"vb7NfD4BD7I_"},"source":["## Transformer POS Tagging Model"]},{"cell_type":"markdown","metadata":{"id":"xRLmOHQzE4He"},"source":["Your task is to implement the Transformer architecture (https://arxiv.org/pdf/1706.03762.pdf) and apply it to tagging.\n","\n","Here is a diagram of the architecture you will implement:\n","\n","<img src=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\" width=\"180em\">\n","\n","This portion is referred to as the \"Transformer Encoder\". In the paper there is also a decoder portion for generating text one token at a time; such a decoder is not needed for this project.\n","\n","The key elements of the Transformer are a multi-headed attention mechanism, and a feed-forward layer.\n","\n","Each sub-layer (whether multi-head attention of feed forward) uses a residual connection followed by Layer Normalization (`nn.LayerNorm` in pytorch). Both residual connections and normalizations are crucial to being able to train a model that's more than a couple layers deep.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xbL92ahgw09l"},"source":["The first part is multi-head self-attention. In this layer, you will need to:\n","- Apply linear projections to convert the feature vector at each token into separate vectors for the query, key, and value.\n","- Apply attention, scaling the logits by $\\frac{1}{sqrt(d_{qkv})}$.\n","- Ensure proper masking, such that padding tokens are never attended to.\n","- Perform attention `n_head` times in parallel, where the results are concatenated and then projected using a linear layer.\n","\n","<img src=\"https://www.researchgate.net/publication/332139525/figure/fig3/AS:743081083158528@1554175744311/a-The-Transformer-model-architecture-b-left-Scaled-Dot-Product-Attention.ppm\" width=\"360em\">\n","\n","You should include two types of dropout in your code (with probability set by the  `dropout` argument):\n","- Dropout should be applied to the output of the attention layer (just prior to the residual connection, denoted by \"Add & Norm\" in the first figure)\n","- Dropout should *also* be applied to attention probabilites, right after the softmax operation that's applied to query-key dot products. This type of dropout stochastically prevents a query position from attending to a fraction of key positions, which can help generalization. (Note that the probabilities will no longer sum to 1, but that's okay - they will still have an expectation of 1 due to PyTorch's dropout rescaling)\n","\n","Notes:\n","- Query vector should have shape `[batch size, n_head, sequence_len, d_k]`\n","- Key vector should have shape `[batch size, n_head, n_word, d_k]`\n","- Value vector should have shape `[batch size, n_head, n_word, d_v]`\n","- Output vector of scaled dot-product attention should have shape `[batch_size, n_head, n_field, n_dim]`\n","- Vaswani et al. define the output of the attention layer as concatenating the various heads and then multiplying by a matrix $W^O$. It's also possible to implement this as a sum without ever calling `torch.cat`: note that $\\text{Concat}(head_1, \\ldots, head_h)W^O = head_1 W^O_1 + \\ldots + head_h W^O_h$ where $W^O = \\begin{bmatrix} W^O_1\\\\ \\vdots\\\\ W^O_h\\end{bmatrix}$"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"IZ4v4TKJXzdE","trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model=256, d_k=32, d_v=32, n_head=4, dropout=0.1, **kwargs):\n","        super().__init__(**kwargs)\n","        self.d_model = d_model\n","        self.d_k = d_k\n","        self.d_v = d_v\n","        self.n_head = n_head\n","\n","        # We provide these model parameters to give an example of a weight\n","        # initialization approach that we know works well for our tasks. Feel free\n","        # to delete these lines and write your own, if you would like.\n","        self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_k))\n","        self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_k))\n","        self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_v))\n","        self.w_o = nn.Parameter(torch.Tensor(n_head, d_v, d_model))\n","        nn.init.xavier_normal_(self.w_q)\n","        nn.init.xavier_normal_(self.w_k)\n","        nn.init.xavier_normal_(self.w_v)\n","        nn.init.xavier_normal_(self.w_o)\n","\n","        # The hyperparameters given as arguments to this function and others\n","        # should be sufficient to reach the score targets for this project\n","\n","        \"\"\"YOUR CODE HERE\"\"\"\n","\n","        # BEGIN SOLUTION\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # END SOLUTION\n","\n","    def forward(self, q, k, v, mask):\n","        \"\"\"Runs the multi-head self-attention layer.\n","\n","        Args:\n","            q: query representation, a tensor of shape [batch size, sequence_len, d_model]\n","            k: key representation, a tensor of shape [batch size, n_word, d_model]\n","            v: value representation, a tensor of shape [batch size, n_word, d_model]\n","            mask: a mask for disallowing attention to padding tokens. You will need to\n","                  construct the mask yourself further on in this notebook. You may\n","                  implement masking in any way; there is no requirement that you use\n","                  a particular form for the mask object.\n","        Returns:\n","            A single tensor containing the output from this layer\n","        \"\"\"\n","        \"\"\"YOUR CODE HERE\"\"\"\n","        # Implementation tip: using torch.einsum will greatly simplify the code that\n","        # you need to write.\n","\n","        # BEGIN SOLUTION\n","\n","        # Linear\n","        query = torch.einsum('hmk,bsm->bhsk', self.w_q, q)                              # [batch size, n_head, sequence_len, d_k]\n","        key = torch.einsum('hmk,bwm->bhwk', self.w_k, k)                                # [batch size, n_head, n_word, d_k]\n","        value = torch.einsum('hmv,bwm->bhwv', self.w_v, v)                              # [batch size, n_head, n_word, d_v]\n","\n","        # MatMul\n","        att = torch.einsum('bhsk,bhwk->bhsw', query, key)                               # [batch size, n_head, sequence_len, n_word]\n","        # Scale\n","        att = att / self.d_k**0.5\n","        # Mask\n","        att = att*(~mask[:, None, None, :]) + (-1e9)*(mask[:, None, None, :])\n","        # Softmax\n","        att = self.dropout(torch.softmax(att, dim=-1))\n","        # MatMul\n","        att = torch.einsum('bhsw,bhwv->bhsv', att, value)                               # [batch size, n_head, sequence_len, d_v]\n","\n","        # Concat + Linear\n","        out = torch.sum(torch.einsum('bhsv,hvm->bhsm', att, self.w_o), 1)               # [batch size, sequence_len, d_model]\n","\n","        return self.dropout(out)\n","\n","        # END SOLUTIOIN"]},{"cell_type":"markdown","metadata":{"id":"BSLWy_8Ne82f"},"source":["The other component is the position-wise feed forward layer. This layer's architecture is sometimes called dense-relu-dense, because it consists of two dense linear layers with ReLU nonlinearity in the middle. The dropout here is typically applied at the output of the layer instead of next to the non-linearity in the middle."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"aYq9acgnXvhk","trusted":true},"outputs":[],"source":["class PositionwiseFeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super().__init__()\n","        \"\"\"YOUR CODE HERE\"\"\"\n","\n","        # BEGIN SOLUTION\n","\n","        self.network = nn.Sequential(\n","            nn.Linear(d_model, d_ff),\n","            nn.ReLU(),\n","            nn.Linear(d_ff, d_model),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # END SOLUTION\n","\n","    def forward(self, x):\n","        \"\"\"YOUR CODE HERE\"\"\"\n","\n","        # BEGIN SOLUTION\n","\n","        return self.network(x)\n","\n","        # END SOLUTION"]},{"cell_type":"markdown","metadata":{"id":"Gm0-10wAfA5N"},"source":["Combining the two gives the full transformer encoder architecture."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"CahKqIbqKDTf","trusted":true},"outputs":[],"source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, d_model=256, d_ff=1024, n_layers=4, n_head=4, d_k=32, d_v=32,\n","                 dropout=0.1):\n","        super().__init__()\n","        # Implementation tip: if you are storing nn.Module objects in a list, use\n","        # nn.ModuleList. If you use assignment statements of the form\n","        # `self.sublayers = [x, y, z]` with a plain python list instead of a\n","        # ModuleList, you might find that none of the sub-layer parameters are\n","        # trained.\n","\n","        \"\"\"YOUR CODE HERE\"\"\"\n","\n","        # BEGIN SOLUTION\n","\n","        self.attentions = nn.ModuleList(\n","            [MultiHeadAttention(d_model, d_k, d_v, n_head, dropout) for _ in range(n_layers)]\n","        )\n","        self.attention_norms = nn.ModuleList(\n","            [nn.LayerNorm(d_model) for _ in range(n_layers)]\n","        )\n","\n","        self.feedforwards = nn.ModuleList(\n","            [PositionwiseFeedForward(d_model, d_ff, dropout) for _ in range(n_layers)]\n","        )\n","        self.feedforward_norms = nn.ModuleList(\n","            [nn.LayerNorm(d_model) for _ in range(n_layers)]\n","        )\n","\n","        # END SOLUTION\n","\n","    def forward(self, x, mask):\n","        \"\"\"Runs the Transformer encoder.\n","\n","        Args:\n","            x: the input to the Transformer, a tensor of shape\n","               [batch size, length, d_model]\n","            mask: a mask for disallowing attention to padding tokens. You will need to\n","                  construct the mask yourself further on in this notebook. You may\n","                  implement masking in any way; there is no requirement that you use\n","                  a particular form for the mask object.\n","        Returns:\n","            A single tensor containing the output from the Transformer\n","        \"\"\"\n","\n","        \"\"\"YOUR CODE HERE\"\"\"\n","\n","        # BEGIN SOLUTION\n","\n","        for attention, attention_norm, feedforward, feedforward_norm in zip(\n","            self.attentions, self.attention_norms, self.feedforwards, self.feedforward_norms\n","        ):\n","            out = attention(x, x, x, mask)\n","            x = attention_norm(out + x)\n","            out = feedforward(x)\n","            x = feedforward_norm(out + x)\n","        return x\n","\n","        # END SOLUTION"]},{"cell_type":"markdown","metadata":{"id":"m2mLail6UMjN"},"source":["Unlike with recurrent neural networks, word order is not encoded in the Transformer architecture directly. Instead, positions of words are provided in the form of position embeddings that are added to the feature vector of each word. The exact formulation of the position embeddings tends to be implementation-dependent, and a number of approaches have been proposed in the literature. The `AddPositionalEncoding` class below provides a version of positional encoding, with dropout, that we found to work well for parsing (the second task in this assignment). However, you are allowed to use a different implementation if you would like."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"OQx5ZmlZt0H2","trusted":true},"outputs":[],"source":["class AddPositionalEncoding(nn.Module):\n","    def __init__(self, d_model=256, input_dropout=0.1, timing_dropout=0.1,\n","                 max_len=512):\n","        super().__init__()\n","        self.timing_table = nn.Parameter(torch.FloatTensor(max_len, d_model))\n","        nn.init.normal_(self.timing_table)\n","        self.input_dropout = nn.Dropout(input_dropout)\n","        self.timing_dropout = nn.Dropout(timing_dropout)\n","    \n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: A tensor of shape [batch size, length, d_model]\n","        \"\"\"\n","        x = self.input_dropout(x)\n","        timing = self.timing_table[None, :x.shape[1], :]\n","        timing = self.timing_dropout(timing)\n","        return x + timing"]},{"cell_type":"markdown","metadata":{"id":"RK6B7pvEVl-H"},"source":["Complete the `TransformerPOSTaggingModel` below. Your goal is to achieve a target of 95% accuracy, or higher, on the validation set."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"inIqeRH6VlfW","trusted":true},"outputs":[],"source":["class TransformerPOSTaggingModel(POSTaggingModel):\n","    def __init__(self, d_model=256):\n","        super().__init__()\n","        self.add_timing = AddPositionalEncoding(d_model)\n","        self.encoder = TransformerEncoder(d_model)\n","        \"\"\"YOUR CODE HERE.\"\"\"\n","\n","        # BEGIN SOLUTION\n","\n","        self.embedding = nn.Embedding(VOCAB.GetPieceSize(), d_model)\n","        self.linear = nn.Linear(d_model, len(PARTS_OF_SPEECH))\n","\n","        # END SOLUTION\n","\n","    def encode(self, batch):\n","        \"\"\"\n","        Args:\n","            batch: an input batch as a dictionary; the key 'ids' holds the vocab ids\n","                of the subword tokens in a tensor of size [batch_size, sequence_length]\n","        Returns:\n","            A single tensor containing logits for each subword token\n","                You don't need to filter the unlabeled subwords - this is handled by our\n","                code above.\n","        \"\"\"\n","\n","        # Implementation tip: you will want to use another normalization layer\n","        # between the output of the encoder and the final projection layer\n","\n","        \"\"\"YOUR CODE HERE.\"\"\"\n","\n","        # BEGIN SOLUTION\n","\n","        embeddings = self.embedding(batch['ids'])\n","        mask = (batch['ids'] == PAD_ID)\n","\n","        out = self.encoder(self.add_timing(embeddings), mask)\n","        return self.linear(out)\n","\n","        # END SOLUTION"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"5o812OQHufeg","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0acb090882bf4ccd89b71bf8e60967c1","version_major":2,"version_minor":0},"text/plain":["training:   0%|          | 0/8 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3cd83eeb21ad4c8e8f9d4018fe87f8c2","version_major":2,"version_minor":0},"text/plain":["epoch 1:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Obtained a new best validation metric of 0.923, saving model checkpoint to ./saves/tagging_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7647fdc8cdbe488fba9185b3d02449cb","version_major":2,"version_minor":0},"text/plain":["epoch 2:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Obtained a new best validation metric of 0.937, saving model checkpoint to ./saves/tagging_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c04542fcabc4e69b1fe4ee3d1b08911","version_major":2,"version_minor":0},"text/plain":["epoch 3:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Obtained a new best validation metric of 0.943, saving model checkpoint to ./saves/tagging_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e90294e9991d430597d203938a83bd22","version_major":2,"version_minor":0},"text/plain":["epoch 4:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Obtained a new best validation metric of 0.950, saving model checkpoint to ./saves/tagging_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ab9b4df1f6a4111b51197253a80f2f1","version_major":2,"version_minor":0},"text/plain":["epoch 5:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Obtained a new best validation metric of 0.954, saving model checkpoint to ./saves/tagging_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d636edd2b614f1c82fc378409f7e525","version_major":2,"version_minor":0},"text/plain":["epoch 6:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Obtained a new best validation metric of 0.956, saving model checkpoint to ./saves/tagging_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f2a3504575d40dbbe3d7940b59fbec6","version_major":2,"version_minor":0},"text/plain":["epoch 7:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Obtained a new best validation metric of 0.957, saving model checkpoint to ./saves/tagging_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f8f63a508484e5ead42acf4e2747571","version_major":2,"version_minor":0},"text/plain":["epoch 8:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Obtained a new best validation metric of 0.957, saving model checkpoint to ./saves/tagging_model.pt...\n","Reloading best model checkpoint from ./saves/tagging_model.pt...\n"]}],"source":["# You are welcome to adjust these parameters based on your model implementation.\n","num_epochs = 8\n","batch_size = 16\n","\n","tagging_model = TransformerPOSTaggingModel().to(device)\n","train(tagging_model, num_epochs, batch_size, \"./saves/tagging_model.pt\")"]},{"cell_type":"markdown","metadata":{"id":"_CQVsw0yhhne"},"source":["*You should download the `tagging_model.pt` file now so you can use it for generating submission files below even if your session gets disconnected before you finish.*"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":261,"status":"ok","timestamp":1623972393256,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"_yPpbQ6wWFto","outputId":"8ccb6da7-3a64-430f-ecb7-16b2e98f50bd","trusted":true},"outputs":[{"data":{"text/plain":["[[('Influential', 'JJ'),\n","  ('members', 'NNS'),\n","  ('of', 'IN'),\n","  ('the', 'DT'),\n","  ('House', 'NNP'),\n","  ('Ways', 'NNP'),\n","  ('and', 'CC'),\n","  ('Means', 'NNP'),\n","  ('Committee', 'NNP'),\n","  ('introduced', 'VBD'),\n","  ('legislation', 'NN'),\n","  ('that', 'WDT'),\n","  ('would', 'MD'),\n","  ('restrict', 'VB'),\n","  ('how', 'WRB'),\n","  ('the', 'DT'),\n","  ('new', 'JJ'),\n","  ('savings-and-loan', 'JJ'),\n","  ('bailout', 'NN'),\n","  ('agency', 'NN'),\n","  ('can', 'MD'),\n","  ('raise', 'VB'),\n","  ('capital', 'NN'),\n","  (',', ','),\n","  ('creating', 'VBG'),\n","  ('another', 'DT'),\n","  ('potential', 'JJ'),\n","  ('obstacle', 'NN'),\n","  ('to', 'TO'),\n","  ('the', 'DT'),\n","  ('government', 'NN'),\n","  (\"'s\", 'POS'),\n","  ('sale', 'NN'),\n","  ('of', 'IN'),\n","  ('sick', 'JJ'),\n","  ('thrifts', 'NNS'),\n","  ('.', '.')]]"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["predict_tags(tagging_model, 'dev', limit=1)"]},{"cell_type":"markdown","metadata":{"id":"u15C5gGLMVSa"},"source":["## Parsing: Task Setup"]},{"cell_type":"markdown","metadata":{"id":"R7I01Tl1MYR9"},"source":["Next, let's move on from predicting tags to predicting full syntax trees. Let's start by taking a look at an example tree and the `nltk.tree.Tree` class."]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1623972393257,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"YAj1lz-w_Hxq","outputId":"e8413214-7b42-4766-f5b3-29d678ae5d87","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(TOP\n","  (S\n","    (NP (PRP She))\n","    (VP (VBZ enjoys) (S (VP (VBG playing) (NP (NN tennis)))))\n","    (. .)))\n"]},{"data":{"image/svg+xml":"<svg baseProfile=\"full\" height=\"360px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,264.0,360.0\" width=\"264px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">TOP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"15.1515%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PRP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">She</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.57576%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"75.7576%\" x=\"15.1515%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"32%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">enjoys</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"68%\" x=\"32%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBG</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">playing</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tennis</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.0303%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.09091%\" x=\"90.9091%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.4545%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>","text/plain":["Tree('TOP', [Tree('S', [Tree('NP', [Tree('PRP', ['She'])]), Tree('VP', [Tree('VBZ', ['enjoys']), Tree('S', [Tree('VP', [Tree('VBG', ['playing']), Tree('NP', [Tree('NN', ['tennis'])])])])]), Tree('.', ['.'])])])"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["example_tree = nltk.tree.Tree.fromstring(\"(TOP (S (NP (PRP She)) (VP (VBZ enjoys) (S (VP (VBG playing) (NP (NN tennis))))) (. .)))\")\n","print(example_tree)\n","example_tree"]},{"cell_type":"markdown","metadata":{"id":"6n2YvVNSfkus"},"source":["The approach in this project is to treat parsing as a span classification task. Each span in the sentence (that is, each combination of start and end position) will be assigned a label. Constituents will be labeled with their syntactic category, while non-constituents will recieve a special null label.\n","\n","In the tree above, \"enjoys playing tennis\" will be assigned the label \"VP\", while \"She enjoys playing\" will be assigned the null label.\n","\n","However, there is a slight issue applying this to the tree above: the span \"playing tennis\" is simultaneously a verb phrase (VP) and a nested clause (S). To resolve this issue, we introduce a special chain label \"S+VP\" for this situation.\n","\n","The function `collapse_unary_strip_pos` transforms trees to collapse such unary chains. It also strips part-of-speech labels (which can be predicted by the tagger in the previous part of this project), as well as the root label \"TOP\"."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"XVMG0H5c_SkK","trusted":true},"outputs":[],"source":["def collapse_unary_strip_pos(tree):\n","    def strip_pos(tree):\n","        if len(tree) == 1 and isinstance(tree[0], str):\n","            return tree[0]\n","        else:\n","            return nltk.tree.Tree(tree.label(), [strip_pos(child) for child in tree])\n","    collapsed_tree = strip_pos(tree)\n","    collapsed_tree.collapse_unary(collapsePOS=True)\n","    if collapsed_tree.label() == 'TOP' and len(collapsed_tree) == 1:\n","        collapsed_tree = collapsed_tree[0]\n","    return collapsed_tree "]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1623972393258,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"7cW56XoA__q9","outputId":"4b75f56e-6b5c-40ce-cf01-80069afc8992","trusted":true},"outputs":[{"data":{"image/svg+xml":"<svg baseProfile=\"full\" height=\"216px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,264.0,216.0\" width=\"264px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"15.1515%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">She</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.57576%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"75.7576%\" x=\"15.1515%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"32%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">enjoys</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"16%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"68%\" x=\"32%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S+VP</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">playing</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tennis</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.0303%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.09091%\" x=\"90.9091%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.4545%\" y1=\"1.2em\" y2=\"3em\" /></svg>","text/plain":["Tree('S', [Tree('NP', ['She']), Tree('VP', ['enjoys', Tree('S+VP', ['playing', Tree('NP', ['tennis'])])]), '.'])"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["collapsed_tree = collapse_unary_strip_pos(example_tree)\n","collapsed_tree"]},{"cell_type":"markdown","metadata":{"id":"XEPtgUl2LGVu"},"source":["Tree objects behaves like lists, in that they can be indexed to produce child nodes. Calling `.label()` returns its label.  If a child is a word instead of a subtree node, it will be a Python string."]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":212,"status":"ok","timestamp":1623972393462,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"tc6GfgWJygrv","outputId":"7578d5be-1a5c-47a5-fdb6-fcd252fcdabd","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Child 0 is: (NP She)\n"]},{"data":{"image/svg+xml":"<svg baseProfile=\"full\" height=\"72px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,40.0,72.0\" width=\"40px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">She</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>","text/plain":["Tree('NP', ['She'])"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Child 1 is: (VP enjoys (S+VP playing (NP tennis)))\n"]},{"data":{"image/svg+xml":"<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,200.0,168.0\" width=\"200px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"32%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">enjoys</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"16%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"68%\" x=\"32%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S+VP</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">playing</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tennis</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66%\" y1=\"1.2em\" y2=\"3em\" /></svg>","text/plain":["Tree('VP', ['enjoys', Tree('S+VP', ['playing', Tree('NP', ['tennis'])])])"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Child 1 label is: VP\n"]}],"source":["print('Child 0 is:', collapsed_tree[0])\n","display(collapsed_tree[0])\n","print('Child 1 is:', collapsed_tree[1])\n","display(collapsed_tree[1])\n","print('Child 1 label is:', collapsed_tree[1].label())"]},{"cell_type":"markdown","metadata":{"id":"AQ9UFCYqLjAZ"},"source":["Your first task for parsing will be to implement an `encode_tree` function that maps from Tree objects to sets of spans with labels and starting/end positions. Since we're using a subword tokenization, the span start/end position will be defined in terms of subword positions. The start position is inclusive and the end is exclusive.  (Note that `‚ñÅ` is considered part of the span of the following word even if it is not attached.)\n","\n","Below, we provide an `encode_tree` function that hard-codes the output for our example sentence as an illustration.  We also have code that iterates through the spans returned by `encode_tree` and prints the corresponding fragments from the original tree object."]},{"cell_type":"code","execution_count":39,"metadata":{"id":"weARf84FI3EL","trusted":true},"outputs":[],"source":["def encode_tree(tree):\n","    s = ' '.join(example_tree.leaves())\n","    if s != 'She enjoys playing tennis .':\n","        raise NotImplementedError(\"You should implement encode_tree\")\n","    ids = VOCAB.EncodeAsIds(s)\n","    word_end_mask = [True, False, True, False, True, False, False, True, True]\n","    spans = [\n","        (0, 1, 'NP'), (5, 8, 'NP'), (3, 8, 'S+VP'), (1, 8, 'VP'), (0, 9, 'S')]\n","    return ids, word_end_mask, spans"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1623972393463,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"MvpuRlj4M_-u","outputId":"21d12403-1dce-4f09-d1b7-04e26a3a0c55","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['‚ñÅShe', '‚ñÅenjoy', 's', '‚ñÅplay', 'ing', '‚ñÅ', 't', 'ennis', '‚ñÅ.']\n","\n","NP starting at subword 0 and ending at 1 :\n"]},{"data":{"image/svg+xml":"<svg baseProfile=\"full\" height=\"60px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 8px;\" version=\"1.1\" viewBox=\"0,0,20.0,60.0\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PRP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">She</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>","text/plain":["TreeLayout(Tree('NP', [Tree('PRP', ['She'])]))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","NP starting at subword 5 and ending at 8 :\n"]},{"data":{"image/svg+xml":"<svg baseProfile=\"full\" height=\"60px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 8px;\" version=\"1.1\" viewBox=\"0,0,32.0,60.0\" width=\"32px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tennis</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>","text/plain":["TreeLayout(Tree('NP', [Tree('NN', ['tennis'])]))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","S+VP starting at subword 3 and ending at 8 :\n"]},{"data":{"image/svg+xml":"<svg baseProfile=\"full\" height=\"108px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 8px;\" version=\"1.1\" viewBox=\"0,0,68.0,108.0\" width=\"68px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBG</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">playing</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tennis</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>","text/plain":["TreeLayout(Tree('S', [Tree('VP', [Tree('VBG', ['playing']), Tree('NP', [Tree('NN', ['tennis'])])])]))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","VP starting at subword 1 and ending at 8 :\n"]},{"data":{"image/svg+xml":"<svg baseProfile=\"full\" height=\"132px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 8px;\" version=\"1.1\" viewBox=\"0,0,100.0,132.0\" width=\"100px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"32%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">enjoys</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"68%\" x=\"32%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBG</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">playing</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tennis</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66%\" y1=\"1.2em\" y2=\"3em\" /></svg>","text/plain":["TreeLayout(Tree('VP', [Tree('VBZ', ['enjoys']), Tree('S', [Tree('VP', [Tree('VBG', ['playing']), Tree('NP', [Tree('NN', ['tennis'])])])])]))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","S starting at subword 0 and ending at 9 :\n"]},{"data":{"image/svg+xml":"<svg baseProfile=\"full\" height=\"180px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 8px;\" version=\"1.1\" viewBox=\"0,0,132.0,180.0\" width=\"132px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">TOP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"15.1515%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PRP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">She</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.57576%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"75.7576%\" x=\"15.1515%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"32%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">enjoys</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"68%\" x=\"32%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBG</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">playing</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tennis</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.0303%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.09091%\" x=\"90.9091%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.4545%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>","text/plain":["TreeLayout(Tree('TOP', [Tree('S', [Tree('NP', [Tree('PRP', ['She'])]), Tree('VP', [Tree('VBZ', ['enjoys']), Tree('S', [Tree('VP', [Tree('VBG', ['playing']), Tree('NP', [Tree('NN', ['tennis'])])])])]), Tree('.', ['.'])])]))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["piece_ids, _, spans = encode_tree(example_tree)\n","print([VOCAB.IdToPiece(piece_id) for piece_id in piece_ids])\n","print()\n","\n","subtrees = list(example_tree.subtrees())\n","for start, end, label in spans:\n","    s = VOCAB.DecodeIds(piece_ids[start:end])\n","    for subtree in subtrees:\n","        if s == ' '.join(subtree.leaves()):\n","            print(label, 'starting at subword', start, 'and ending at', end, ':')\n","            display(svgling.draw_tree(subtree, font_size=8))\n","            print()\n","            break"]},{"cell_type":"markdown","metadata":{"id":"5dGk0UlgJ4Di"},"source":["Your task is to implement `encode_tree` so that it works for all trees, rather than returning hard-coded results."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"LalYW2FVJyUG","trusted":true},"outputs":[],"source":["def _convert(out, is_word_end):\n","    # BEGIN SOLUTION\n","\n","    index_end = [0]+[i+1 for i, end in enumerate(is_word_end) if end]\n","    return [(index_end[start], index_end[end], label) for start, end, label in out]\n","\n","    # END SOLUTION\n","\n","def encode_tree(tree):\n","    \"\"\"Converts a tree into subword token ids and a list of labeled spans.\n","\n","    Args:\n","        tree: an nltk.tree.Tree object\n","\n","    Returns:\n","        A tuple (ids, is_word_end, spans)\n","            ids: a list of token ids in the subword vocabulary\n","            is_word_end: a list with elements of type bool, where True indicates that\n","                        the word piece at that position is the last within its word.\n","            spans: a list of tuples of the form (start, end, label), where `start` is\n","                  the position in ids where the span starts, `end` is the ending\n","                  point in the span (exclusive), and `label` is a string indicating\n","                  the syntactic label for the constituent.\n","    \"\"\"\n","    tree = collapse_unary_strip_pos(tree)\n","    # Implementation tip: it may help to look at encode_sentence, provided earlier\n","    \"\"\"YOUR CODE HERE\"\"\"\n","\n","    # BEGIN SOLUTION\n","\n","    out = []\n","    def get_spans(node, index, out):\n","        if isinstance(node, str):\n","            return index + 1\n","\n","        start = index\n","        for child in node:\n","            index = get_spans(child, index, out)\n","        out.append((start, index, node.label()))\n","\n","        return index\n","    get_spans(tree, 0, out)\n","\n","    ids, is_word_end = encode_sentence(tree.leaves())\n","    spans = _convert(out, is_word_end)\n","\n","    return ids, is_word_end, spans\n","\n","    # END SOLUTION"]},{"cell_type":"markdown","metadata":{"id":"3JQECbjjL9wV"},"source":["Here is a simple test that your implementation is correct. The `assert` statement should not produce any errors for a correct implementation."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"W1GQ_-sXN_Qr","trusted":true},"outputs":[],"source":["for tree in READER.parsed_sents('dev'):\n","    piece_ids, _, spans = encode_tree(tree)\n","    if UNK_ID in piece_ids:\n","        continue  # skip verifying this sentence\n","    span_strs = [VOCAB.DecodeIds(piece_ids[start:end]) for start, end, _ in spans]\n","    assert set([' '.join(subtree.leaves()) for subtree in tree.subtrees()\n","                if not isinstance(subtree[0], str)]) == set(span_strs)"]},{"cell_type":"markdown","metadata":{"id":"ssrIa50sKUKi"},"source":["Now we can take inventory of all span labels in the training data. The validation and test sets have a few span labels outside this set because collapsing unary chains creates additional labels, so we also introduce an `UNK` label. Finally, there is a null label to represent that a span is not a syntactic constituent."]},{"cell_type":"code","execution_count":43,"metadata":{"id":"NTWsre3cTJyR","trusted":true},"outputs":[],"source":["SPAN_LABELS = set()\n","for tree in READER.parsed_sents('train'):\n","    _, _, spans = encode_tree(tree)\n","    for _, _, label in spans:\n","        SPAN_LABELS.add(label)\n","SPAN_LABELS = ['', 'UNK'] + sorted(SPAN_LABELS)\n","\n","# This is another check to ensure that your implementation matches ours.\n","assert len(SPAN_LABELS) == 114"]},{"cell_type":"markdown","metadata":{"id":"u8fEciiRLLHT"},"source":["The implementation of the `ParsingDataset` class below is left for you to complete, based on the requirements of your model. You may wish to refer to the earlier `POSTaggingDataset` when implementing this class."]},{"cell_type":"code","execution_count":44,"metadata":{"id":"jGOw6mS_JH67","trusted":true},"outputs":[],"source":["class ParsingDataset(torch.utils.data.Dataset):\n","    def __init__(self, split):\n","        assert split in ('train', 'dev', 'test')\n","        self.trees = READER.parsed_sents(split)\n","        if split == 'train':\n","            # To speed up training, we only train on short sentences.\n","            self.trees = [tree for tree in self.trees if len(tree.leaves()) <= 40]\n","\n","    def __len__(self):\n","        return len(self.trees)\n","\n","    def __getitem__(self, index):\n","        \"\"\" This function loads a single tree into tensors for 'ids', 'labels', and\n","        'is_word_end'.\n","\n","        See 'collate' function below for a description of the batched version of the\n","        tensors to return.\n","        \"\"\"\n","\n","        \"\"\"YOUR CODE HERE\"\"\"\n","\n","        # don't forget to add BOS_ID and EOS_ID to the start and end of the sentence\n","        \n","        # note that you will need to check for unknown labels and replace them with\n","        #  'UNK' (because of unique unary chains in the validation and test sets)\n","\n","        # use '' (index 0) as the null label for spans that aren't constituents\n","        # you will need a separate index (such as -1) for positions that should not\n","        #  receive loss\n","\n","        # BEGIN SOLUTION\n","\n","        tree = self.trees[index]\n","\n","        ids, is_word_end, spans = encode_tree(tree)\n","\n","        ids = [BOS_ID] + ids + [EOS_ID]\n","        ids = torch.tensor(ids)\n","\n","        is_word_end = [False] + is_word_end + [False]\n","        is_word_end = torch.tensor(is_word_end)\n","\n","        label_id = dict(zip(SPAN_LABELS, range(len(SPAN_LABELS))))\n","        labels = torch.tril(torch.full((len(ids), len(ids)), -1, dtype=torch.long))\n","        labels[0, :] = -1\n","        for start, end, label in spans:\n","            if label not in label_id:\n","                label = 'UNK'\n","            labels[start+1, end+1] = label_id[label]\n","\n","        return {'ids': ids, 'is_word_end': is_word_end, 'labels': labels}\n","\n","        # END SOLTUION\n","\n","    @staticmethod\n","    def collate(batch):\n","        \"\"\" This function takes a list of examples as output by your __getitem__\n","        function and turns them into batched tensors.\n","        \n","        Returns:\n","            A dictionary with three keys.\n","            * 'ids' holds a tensor of shape [batch_size, max_sentence_length] and\n","                dtype torch.long (where max length is taken within this batch).\n","            * 'labels' is a required feature that's used by our evaluation logic. It\n","                should be a torch.long tensor of shape\n","                [batch_size, max_sentence_length, max_sentence_length] with\n","                labels[batch, i, j] representing the label of the span starting at\n","                subword position i and ending at subword position j (exclusive).\n","            * 'is_word_end' is a required feature that's used by our skeleton code.\n","                It should be a torch.bool tensor of shape [batch_size, max_sentence_length],\n","                with True values at the last sub-word piece for each word.\n","        \"\"\"\n","\n","        \"\"\"YOUR CODE HERE\"\"\"\n","\n","        # BEGIN SOLUTION\n","\n","        ids = pad_sequence(\n","            [item['ids'] for item in batch],\n","            batch_first=True, padding_value=PAD_ID)\n","\n","        is_word_end = pad_sequence(\n","            [item['is_word_end'] for item in batch],\n","            batch_first=True, padding_value=False)\n","\n","        labels = torch.full((len(ids), ids.shape[-1], ids.shape[-1]), -1, dtype=torch.long)\n","        for i, item in enumerate(batch):\n","            labels[i, :len(item['ids']), :len(item['ids'])] = item['labels']\n","\n","        return {\n","            'ids': ids.to(device),\n","            'labels': labels.to(device),\n","            'is_word_end': is_word_end.to(device),\n","        }\n","\n","        # END SOLUTION"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":10473,"status":"ok","timestamp":1623972418480,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"WVvc7HMrShBl","outputId":"f2ae5f99-e849-40d9-e793-be80c3899852","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['Ms.', 'Haag', 'plays', 'Elianti', '.']\n"]},{"data":{"text/plain":["{'ids': tensor([   1,  126,    4,   14, 9362,  711,    4, 4386, 8675,    7,    2]),\n"," 'is_word_end': tensor([False, False, False,  True,  True, False,  True, False,  True,  True,\n","         False]),\n"," 'labels': tensor([[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n","         [-1, -1,  0,  0,  0, 32,  0,  0,  0,  0, 67],\n","         [-1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0],\n","         [-1, -1, -1, -1,  0,  0,  0,  0,  0,  0,  0],\n","         [-1, -1, -1, -1, -1,  0,  0,  0,  0,  0,  0],\n","         [-1, -1, -1, -1, -1, -1,  0,  0,  0, 94,  0],\n","         [-1, -1, -1, -1, -1, -1, -1,  0,  0,  0,  0],\n","         [-1, -1, -1, -1, -1, -1, -1, -1,  0, 32,  0],\n","         [-1, -1, -1, -1, -1, -1, -1, -1, -1,  0,  0],\n","         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0],\n","         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])}"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["dataset_for_inspection = ParsingDataset('train')\n","print(dataset_for_inspection.trees[0].leaves())\n","dataset_for_inspection[0]"]},{"cell_type":"markdown","metadata":{"id":"itUNYvsUJg9d"},"source":["The cell below sanity-checks your implementation of the `labels` and `is_word_end` features of the dataset. The tests are not exhaustive, but any errors are an indication that you have not constructed the batched examples correctly."]},{"cell_type":"code","execution_count":46,"metadata":{"id":"qVp6aSizEDHR","trusted":true},"outputs":[],"source":["dataset_for_check = ParsingDataset('dev')\n","batch_size_for_check = 16\n","data_loader_for_test = torch.utils.data.DataLoader(\n","    dataset_for_check, batch_size=batch_size_for_check, shuffle=False,\n","    collate_fn=dataset_for_check.collate)\n","\n","for batch_num, batch in enumerate(data_loader_for_test):\n","    for example_num in range(batch['ids'].shape[0]):\n","        tree = dataset_for_check.trees[\n","            batch_size_for_check * batch_num + example_num]\n","        piece_ids = batch['ids'][example_num].cpu().numpy().tolist()\n","        labels = batch['labels'][example_num].cpu().numpy()\n","        is_word_end = batch['is_word_end'][example_num].cpu().numpy()\n","\n","        if UNK_ID in piece_ids:\n","            continue  # skip verifying this sentence\n","\n","        constituents = set([\n","            ' '.join(subtree.leaves())\n","            for subtree in tree.subtrees() if not isinstance(subtree[0], str)])\n","        expected_labels = [\n","            subtree.label()\n","            for subtree in collapse_unary_strip_pos(tree).subtrees()]\n","\n","        labels_from_dataset = []\n","        for i in range(labels.shape[0]):\n","            for j in range(labels.shape[1]):\n","                span_str = VOCAB.DecodeIds(piece_ids[i:j])\n","                try:\n","                    if not piece_ids[i:j]:\n","                        assert labels[i, j] == -1, (\n","                            'This range is not a span and must have label -1')\n","                    elif (BOS_ID in piece_ids[i:j]\n","                          or EOS_ID in piece_ids[i:j]\n","                          or PAD_ID in piece_ids[i:j]\n","                    ):\n","                        assert labels[i, j] == -1, (\n","                            'This span contains a start, stop, or padding token and must have '\n","                            'label -1')\n","                    elif span_str not in constituents:\n","                        assert labels[i, j] == 0, (\n","                            'This span is not a constituent, but its label is not 0')\n","                    else:\n","                        # The span may be a constituent (or this may be a false positive,\n","                        # because this verification code only looks at string equality)\n","                        assert 0 <= labels[i, j] < len(SPAN_LABELS), (\n","                            'The assigned label is not valid for this span')\n","                        if labels[i, j] != 0:\n","                            labels_from_dataset.append(SPAN_LABELS[labels[i, j]])\n","\n","                    if labels[i, j] > 0:\n","                        assert i == 0 or piece_ids[i-1] == BOS_ID or is_word_end[i-1], (\n","                            'This labeled span does not start on a word boundary')\n","                        assert j == 0 or piece_ids[j-1] == BOS_ID or is_word_end[j-1], (\n","                            'This labeled span does not end on a word boundary')\n","                except AssertionError as e:\n","                    print(f'ERROR at span [i = {i}, j = {j}, label = {labels[i,j]}]')\n","                    print(str(e))\n","                    print()\n","                    for i, (piece_id, is_word_end_bool) in enumerate(\n","                        zip(piece_ids, is_word_end)\n","                    ):\n","                        print('{:2d} {: <10} {}'.format(\n","                            i, \"(word end)\" if is_word_end_bool else \"\",\n","                            VOCAB.IdToPiece(piece_id)))\n","                    raise\n","\n","        assert sorted(expected_labels) == sorted(labels_from_dataset), (\n","            f'The dataset has {len(labels_from_dataset)} non-empty labels for this '\n","            f'example, which do not match the {len(expected_labels)} labels in the '\n","            f'original tree')"]},{"cell_type":"markdown","metadata":{"id":"TPWrN4Eb2lN5"},"source":["## Parsing: Model"]},{"cell_type":"markdown","metadata":{"id":"DcmWjcttMRck"},"source":["Next, you will implement a Transformer-based parsing model. Below is a base class that checks performance on the validation set based on a local decision at each span.  Later, you will implement CKY decoding that ensures the output is a tree rather than a set of possibly-intersecting spans."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"kcT3t6lVz2ij","trusted":true},"outputs":[],"source":["class ParsingModel(nn.Module):\n","    def encode(self, batch):\n","        # you will override this below\n","        raise NotImplementedError()\n","\n","    def compute_loss(self, batch):\n","        # you will override this below\n","        raise NotImplementedError()\n","\n","    def get_validation_metric(self):\n","        dataset = ParsingDataset('dev')\n","        data_loader = torch.utils.data.DataLoader(\n","            dataset, batch_size=8, collate_fn=dataset.collate)\n","        self.eval()\n","        total_gold_spans = 0\n","        total_predicted_spans = 0\n","        total_correct = 0\n","        with torch.no_grad():\n","            for batch in data_loader:\n","                mask = (batch['labels'] != -1)\n","                model_output = self.encode(batch)\n","                predicted_labels = model_output.argmax(-1)\n","                predicted_labels = predicted_labels[mask]\n","                gold_labels = batch['labels'][mask]\n","\n","                total_gold_spans += (gold_labels != 0).sum().item()\n","                total_predicted_spans += (predicted_labels != 0).sum().item()\n","                total_correct += ((predicted_labels == gold_labels) & (gold_labels != 0)\n","                    ).sum().item()\n","\n","        if total_predicted_spans != 0:\n","            precision = total_correct / total_predicted_spans\n","        else:\n","            precision = 0.0\n","        recall = total_correct / total_gold_spans\n","        if precision == 0.0 or recall == 0.0:\n","            f1 = 0.0\n","        else:\n","            f1 = 2 * precision * recall / (precision + recall)\n","        # For convenience, we represent precion/recall/F1 as percentage points.\n","        precision *= 100\n","        recall *= 100\n","        f1 *= 100\n","        print(f\"precision={precision:.2f} recall={recall:.2f} f1={f1:.2f}\")\n","        return f1"]},{"cell_type":"markdown","metadata":{"id":"XgUDTDfSMh4O"},"source":["Here is a skeleton for the parsing model that you should fill in. The parser should:\n","- Run a Transformer encoder to produce a vector at each position in the sentence.\n","- Compute a vector for each span, by subtracting the vectors for the start and endpoints. You may also try other options, such as adding, averaging, or concatenating.\n","- Run an MLP span classifier that takes these span vectors as input. The MLP should have one layer of nonlinearity. In our implementation, the MLP also includes a Layer Normalization step."]},{"cell_type":"code","execution_count":48,"metadata":{"id":"X75QefU4N_jj","trusted":true},"outputs":[],"source":["class TransformerParsingModel(ParsingModel):\n","    def __init__(self, d_model=256):\n","        super().__init__()\n","        self.add_timing = AddPositionalEncoding(d_model)\n","        self.encoder = TransformerEncoder(d_model)\n","        \"\"\"YOUR CODE HERE.\"\"\"\n","\n","        # BEGIN SOLUTION\n","\n","        self.embedding = nn.Embedding(VOCAB.GetPieceSize(), d_model)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(d_model, d_model),\n","            nn.ReLU(),\n","            nn.LayerNorm(d_model),\n","            nn.Linear(d_model, len(SPAN_LABELS))\n","        )\n","\n","        # END SOLUTION\n","\n","    def encode(self, batch):\n","        \"\"\"Returns logits for each label and each span in the sentence.\n","\n","        Returns:\n","            A float tensor of shape [batch_size, length, length, len(SPAN_LABELS)],\n","            where the element at position [n, i, j, l] represents the score (logit) of\n","            assigning label l to the span beginning at subword position i and ending\n","            at position j (exclusive), for the n-th example in the batch.\n","        \"\"\"\n","\n","        \"\"\"YOUR CODE HERE\"\"\"\n","\n","        # You don't need to worry about is_word_end here or in compute_loss\n","        # You can train with non-end subwords still in place (with null labels) and\n","        # we'll handle adjusting for subwords for you in the evaluation functions\n","\n","        # BEGIN SOLUTION\n","\n","        embeddings = self.embedding(batch['ids'])\n","        mask = (batch['ids'] == PAD_ID)\n","\n","        out = self.encoder(self.add_timing(embeddings), mask)\n","\n","        out = out.unsqueeze(1) - out.unsqueeze(2)\n","        return self.mlp(out)\n","        \n","        # END SOLUTION\n","\n","    def compute_loss(self, batch):\n","        \"\"\"This function should compute a cross-entropy loss for training the model.\n","\n","        Note that labels should be set to -1 wherever there is no classification\n","        decision to make; for example, due to padding or at positions [..., i, j, :]\n","        where i >= j (i.e. the supposed start position is equal to or comes after\n","        the end position).\n","        \"\"\"\n","        logits = self.encode(batch)\n","        labels = batch['labels']\n","\n","        \"\"\"YOUR CODE HERE\"\"\"\n","\n","        # BEGIN SOLUTION\n","\n","        return F.cross_entropy(\n","            logits.view(-1, logits.shape[-1]), labels.view(-1), ignore_index=-1)\n","\n","        # END SOLUTION"]},{"cell_type":"markdown","metadata":{"id":"10cgiAnyFi-A"},"source":["The code below trains the parser.\n","\n","In our implementation, it reports a validation score of 66-70 F1 after a single epoch and 80-82 F1 after 5 epochs. If you observe worse performance, we recommend that you debug and adjust your model rather than waiting for the full 16 epochs to complete. When debugging, be sure to also check your Transformer implementation: a buggy version of the Transformer might work for part of speech tagging, but not for the harder task of parsing.\n","\n","After training completes, our implementation reports 86.4 validation F1. Each epoch takes about 1.5 minutes on a fast GPU instance."]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":385997,"status":"error","timestamp":1623972859017,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":420},"id":"zcZNJ8m-UwRF","outputId":"1a53f0c3-cabb-45b5-af55-f42d5d3b348f","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2673d04779cd45c2995cffc4ef1deec7","version_major":2,"version_minor":0},"text/plain":["training:   0%|          | 0/16 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ccf6924354144f889b0dd4846ce19a2","version_major":2,"version_minor":0},"text/plain":["epoch 1:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=73.35 recall=64.78 f1=68.80\n","Obtained a new best validation metric of 68.799, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"11d84e1fc7a74364a005dc1ce6c41a37","version_major":2,"version_minor":0},"text/plain":["epoch 2:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=77.16 recall=75.95 f1=76.55\n","Obtained a new best validation metric of 76.547, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee5a2257fc1d4468b53be9c1e6722fa9","version_major":2,"version_minor":0},"text/plain":["epoch 3:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=79.67 recall=77.25 f1=78.44\n","Obtained a new best validation metric of 78.444, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08e7cce9e233428c940b338bd5b5cdb9","version_major":2,"version_minor":0},"text/plain":["epoch 4:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=81.28 recall=80.68 f1=80.98\n","Obtained a new best validation metric of 80.979, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"978f27f62fe147a9b3532980a6808713","version_major":2,"version_minor":0},"text/plain":["epoch 5:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=83.37 recall=81.54 f1=82.44\n","Obtained a new best validation metric of 82.444, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"385a828388594e5ea728e52b7dd7f2e8","version_major":2,"version_minor":0},"text/plain":["epoch 6:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=84.45 recall=82.28 f1=83.35\n","Obtained a new best validation metric of 83.351, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"958260eea5e04a67b2b3849a2441a09b","version_major":2,"version_minor":0},"text/plain":["epoch 7:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=85.10 recall=81.80 f1=83.42\n","Obtained a new best validation metric of 83.416, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52f8410b05da4ac29b64ddf7c32ec5b5","version_major":2,"version_minor":0},"text/plain":["epoch 8:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=86.39 recall=83.18 f1=84.75\n","Obtained a new best validation metric of 84.755, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db3c8a865c644c999a0a19033e5b214e","version_major":2,"version_minor":0},"text/plain":["epoch 9:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=86.51 recall=84.24 f1=85.36\n","Obtained a new best validation metric of 85.360, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17da2a2a7bc847309108fd93cb35f5a0","version_major":2,"version_minor":0},"text/plain":["epoch 10:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=86.37 recall=85.41 f1=85.88\n","Obtained a new best validation metric of 85.884, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2360a8eae10542a4984a26c592eb7e9e","version_major":2,"version_minor":0},"text/plain":["epoch 11:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=87.46 recall=85.56 f1=86.50\n","Obtained a new best validation metric of 86.500, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a74e5870db584d82bf7271815a567629","version_major":2,"version_minor":0},"text/plain":["epoch 12:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=87.58 recall=85.92 f1=86.74\n","Obtained a new best validation metric of 86.741, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4249c8f25b9c4ba890fc139556999a97","version_major":2,"version_minor":0},"text/plain":["epoch 13:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=87.26 recall=86.38 f1=86.81\n","Obtained a new best validation metric of 86.815, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60941a5ca92146aba39106bd93775106","version_major":2,"version_minor":0},"text/plain":["epoch 14:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=87.14 recall=86.80 f1=86.97\n","Obtained a new best validation metric of 86.970, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86d338b8c5284cd58ca84966cb701bd1","version_major":2,"version_minor":0},"text/plain":["epoch 15:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=87.26 recall=86.99 f1=87.13\n","Obtained a new best validation metric of 87.127, saving model checkpoint to ./saves/parsing_model.pt...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"271e104b2fb5497185b16be219366b53","version_major":2,"version_minor":0},"text/plain":["epoch 16:   0%|          | 0/2298 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision=87.10 recall=87.02 f1=87.06\n","Reloading best model checkpoint from ./saves/parsing_model.pt...\n"]}],"source":["# You are welcome to adjust these parameters based on your model implementation.\n","num_epochs = 16\n","batch_size = 16\n","\n","parsing_model = TransformerParsingModel().to(device)\n","train(parsing_model, num_epochs, batch_size, \"./saves/parsing_model.pt\",\n","      dataset_cls=ParsingDataset)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tnSnBH-EMnuQ"},"source":["_We recommend that you download `parsing_model.pt` now in case your runtime gets disconnected._"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Having trained a parser, it is now time to have it produce trees. The argmax model predictions are not guaranteed to be a valid tree: some of the spans may intersect with one another, which is not allowed in our syntactic formalism. Your next task is to implement CKY decoding, which will be used to find the highest-scoring tree under the model (i.e. the set of span label assignments with highest probability, among all sets where none of the spans intersect).  You can reference the parsing lecture slides if you aren't sure where to start.  Note that the lecture slides CKY required special handling for unary chains, but you will not need to implement this case because we have collapsed all unary chains into a single label.  Also, while traditional CKY had to maximize over production rules (between a span label and its children's labels), here we can maximize over span labels independently because we aren't modeling interactions between adjacent labels.\n","\n","CKY is designed to handle binary trees, but some productions have greater than two children.  For this project, we will handle non-binary productions by allowing intermediate dummy nodes with a special null label, implemented in the code as an empty string '' at index 0 in SPAN_LABELS.  All non-spans are assigned this label.  You should allow your CKY to select spans with the null label, but then collapse them out when creating the tree.  For example, to create a trinary production \"(A  b c d)\", CKY would select a tree that looks like \"(A  b (null  c d))\" or \"(A  (null  b c) d)\" and then would remove the null nodes to get \"(A  b c d)\" like we wanted.\n","\n","You will find that if you allow CKY to select null labels and you directly use log probabilities as the scores, you will often end up with trees made up entirely of null labels.  This happens because many spans have a very high probability assigned to the null label (because they clearly shouldn't be a constituent), and choosing a maximum probability tree will be biased towards selecting these.  To avoid having high-probability null labels competing with non-null labels, we can normalize our scores in a different way: for each span, we will subtract the score (logit) of the null label from the scores of all the labels (both null and non-null).  This will make all null label scores 0, and scale the other label scores of the span appropriately.  This normalization will make it so that if a local span decision would prefer a non-null label it will have positive score, and thus have higher score than all 0-score null labels.  That way, we will prefer to use spans with non-null labels unless they conflict with eachother.\n","\n","Implementation tip: Don't forget that it is possible for the tree to have labels on spans of length 1.  These labels will go directly above the part of speech label.  Take a look at the example tree at the beginning of \"Parsing: Setup\", which has a NP span label directly above the NN part-of-speech label."]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["def cky_decode(leaves, label_scores_chart):\n","    \"\"\"\n","    Args:\n","        leaves: a list of all tree leaves\n","        label_scores_chart: an array of shape [length, length, len(SPAN_LABELS)],\n","            where the element at position [i, j, l] represents the score (logit) of\n","            assigning label l to the span beginning at whole word position i and\n","            ending at position j (exclusive).\n","    \"\"\"\n","\n","    \"\"\"YOUR CODE HERE\"\"\"\n","    # BEGIN SOLUTION\n","\n","    label_scores_chart -= label_scores_chart[:, :, [0]]\n","\n","    label_index_chart = np.argmax(label_scores_chart, axis=-1)\n","    label_score_chart = np.take_along_axis(\n","        label_scores_chart, label_index_chart[..., None], axis=-1)[...,0]\n","\n","    best_split_chart = np.full_like(label_index_chart, -1)\n","    best_score_chart = np.zeros_like(label_score_chart)\n","\n","    for i in range(1, len(label_score_chart)):\n","        for j in range(i, len(label_score_chart)):\n","            for k in range(j-i+1, j):\n","                split_score = best_score_chart[j-i, k] + best_score_chart[k, j]\n","                if best_score_chart[j-i, j] <= split_score:\n","                    best_score_chart[j-i, j] = split_score\n","                    best_split_chart[j-i, j] = k\n","            best_score_chart[j-i, j] += label_score_chart[j-i, j]\n","\n","    def construct_tree(root_row, root_col):\n","        label = SPAN_LABELS[label_index_chart[(root_row, root_col)]]\n","        split = best_split_chart[(root_row, root_col)]\n","\n","        if label == '' and split == -1:\n","            return [leaves[root_row]]\n","        elif label != '' and split == -1:\n","            return [nltk.tree.Tree(label, [leaves[root_row]])]\n","        elif label == '' and split != -1:\n","            return [*construct_tree(root_row, split), *construct_tree(split, root_col)]\n","        else:\n","            return [nltk.tree.Tree(label,\n","                [*construct_tree(root_row, split), *construct_tree(split, root_col)])]\n","    return construct_tree(0, len(label_score_chart)-1)[0]\n","\n","    # END SOLUTION"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Having implemented the `cky_decode` function, the `predict` function will run your parser on batches from the dataset."]},{"cell_type":"code","execution_count":89,"metadata":{"id":"RGk0toh0OIDK","trusted":true},"outputs":[],"source":["def predict(parsing_model, split, tagging_model=None):\n","    assert split in ('dev', 'test')\n","    if tagging_model is None:\n","        tagged_sents = READER.tagged_sents(split)\n","    else:\n","        tagged_sents = predict_tags(tagging_model, split)\n","\n","    label_scores_charts = predict_span_label_scores(parsing_model, split)\n","\n","    pred_trees = []\n","    for tagged_sent, label_scores_chart in zip(tagged_sents, label_scores_charts):\n","        leaves = [nltk.tree.Tree(tag, [word]) for word, tag in tagged_sent]\n","        tree = cky_decode(leaves, label_scores_chart)\n","        tree = uncollapse_tree(tree)\n","        pred_trees.append(tree)\n","    return pred_trees\n","\n","\n","def predict_span_label_scores(parsing_model, split):\n","    assert split in ('dev', 'test')\n","    dataset = ParsingDataset(split)\n","    data_loader = torch.utils.data.DataLoader(\n","        dataset, batch_size=8, shuffle=False, collate_fn=dataset.collate)\n","    parsing_model.eval()\n","    all_label_scores_charts = []\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            label_scores_charts = parsing_model.encode(batch)\n","            label_scores_charts = F.log_softmax(label_scores_charts, dim=-1) # Not necessary, but okay to keep. \n","            for i in range(batch['ids'].shape[0]):\n","                label_scores_chart = label_scores_charts[i]\n","\n","                # The data pipeline uses is_word_end for consistency with the part of\n","                # speech tagging models, but here we need is_word_start instead. Note\n","                # that because span endpoints use exclusive indexing, the index actually\n","                # points to the first subword in the next word.\n","                is_word_end = batch['is_word_end'][i]\n","                is_word_start = F.pad(is_word_end, (1, -1), value=False)\n","                is_word_start[1] = True\n","\n","                # Extract scores for whole words only, ignoring any model decisions that\n","                # have a span start or end halfway through a word. Evaluation for\n","                # parsing typically uses the ground-truth tokenization from the dataset.\n","                label_scores_chart = label_scores_chart[\n","                    is_word_start, : ,:][:, is_word_start, :]\n","                label_scores_chart = label_scores_chart.cpu().numpy()\n","\n","                all_label_scores_charts.append(label_scores_chart)\n","    return all_label_scores_charts\n","\n","\n","def uncollapse_tree(tree):\n","    if isinstance(tree, str):\n","        return tree\n","    else:\n","        labels = tree.label().split('+')\n","        children = []\n","        for child in tree:\n","            child = uncollapse_tree(child)\n","            if isinstance(child, str) and (len(tree) > 1\n","                                          or labels[-1] not in PARTS_OF_SPEECH):\n","                child = nltk.tree.Tree('UNK', [child])\n","            children.append(child)\n","        for label in labels[::-1]:\n","            children = [nltk.tree.Tree(label, children)]\n","        return children[0]"]},{"cell_type":"markdown","metadata":{"id":"ZF7gyh-HNY7W"},"source":["The code below runs the parser on the validation data and displays one of the trees."]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[{"data":{"image/svg+xml":"<svg baseProfile=\"full\" height=\"552px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,896.0,552.0\" width=\"896px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"22.3214%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"36%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Another</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"18%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"64%\" x=\"36%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">QP</text></svg><svg width=\"18.75%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">$</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.375%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"25%\" x=\"18.75%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">20</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"56.25%\" x=\"43.75%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">billion</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.875%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.1607%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"75%\" x=\"22.3214%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"8.33333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">MD</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">would</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.16667%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"91.6667%\" x=\"8.33333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"5.19481%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VB</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">be</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"2.5974%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"94.8052%\" x=\"5.19481%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"10.9589%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">raised</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.47945%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"89.0411%\" x=\"10.9589%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"13.8462%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">through</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.92308%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"86.1538%\" x=\"13.8462%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"30.3571%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"58.8235%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Treasury</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.4118%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"41.1765%\" x=\"58.8235%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">bonds</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.4118%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.1786%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.35714%\" x=\"30.3571%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.0357%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"64.2857%\" x=\"35.7143%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">SBAR</text></svg><svg width=\"19.4444%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">WHNP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">WDT</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">which</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.72222%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"80.5556%\" x=\"19.4444%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"17.2414%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">pay</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.62069%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"82.7586%\" x=\"17.2414%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"29.1667%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJR</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">lower</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.5833%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"41.6667%\" x=\"29.1667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">interest</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"29.1667%\" x=\"70.8333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">rates</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.4167%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.6207%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.7222%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"67.8571%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.9231%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.4795%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.5974%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"54.1667%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.8214%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.67857%\" x=\"97.3214%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.6607%\" y1=\"1.2em\" y2=\"3em\" /></svg>","text/plain":["Tree('S', [Tree('NP', [Tree('DT', ['Another']), Tree('QP', [Tree('$', ['$']), Tree('CD', ['20']), Tree('CD', ['billion'])])]), Tree('VP', [Tree('MD', ['would']), Tree('VP', [Tree('VB', ['be']), Tree('VP', [Tree('VBN', ['raised']), Tree('PP', [Tree('IN', ['through']), Tree('NP', [Tree('NP', [Tree('NNP', ['Treasury']), Tree('NNS', ['bonds'])]), Tree(',', [',']), Tree('SBAR', [Tree('WHNP', [Tree('WDT', ['which'])]), Tree('S', [Tree('VP', [Tree('VBP', ['pay']), Tree('NP', [Tree('JJR', ['lower']), Tree('NN', ['interest']), Tree('NNS', ['rates'])])])])])])])])])]), Tree('.', ['.'])])"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["predicted_dev_trees = predict(parsing_model, 'dev')\n","predicted_dev_trees[6]"]},{"cell_type":"markdown","metadata":{"id":"TFbnEobgQIWx"},"source":["## Final evaluation"]},{"cell_type":"markdown","metadata":{"id":"gb5gwdPjQ9gi"},"source":["The standard evaluation for parsing is typically performed using the EVALB software (https://nlp.cs.nyu.edu/evalb/). The F1 score it reports is slightly different than what we reported during training. For one, this score is computed over original spans, without collapsing unary chains. For a constituent to be considered correct, its label must match the ground-truth label. EVALB also ignores punctuation when determining whether spans are correct.\n","\n","The metrics reported by EVALB include:\n","- *Bracketing Recall*: Number of correct constituents divided by the number of constituents in the ground-truth data\n","- *Bracketing Precision*: Number of correct constituents divided by the number of constituents in the predicted trees\n","- *Bracketing FMeasure*: The F1 score, which is the harmonic mean of the Bracketing Recall and Bracketing Precision\n","- *Complete Match*: Percentage of sentences where recall and precision are both 100%\n","- *Average crossing*: Number of constituents crossing a ground-truth constituent divided by the number of sentences\n","- *No crossing*: Percentage of sentences which have 0 crossing brackets\n","\n","Metrics are reported both for the full dataset and for the subset of sentences that have length 40 or shorter.\n","\n","Your model should be able to achieve an F1 score (\"Bracketing FMeasure\") **above 89.0 for sentences of length 40 or shorter** (\"len<=40\"). **The autograder will check this number as part of grading.**\n","\n","In the interest of speed, the code in this project only trains on short sentences, and our recommended hyperparameters keep the model size small. Relaxing these restrictions and training for about a day can give better results, possibly as high as 92 F1 on sentences of all lengths."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNePLbPGQkrk","trusted":true},"outputs":[],"source":["# We first need to compile the EVALB program\n","!cd data/EVALB; make "]},{"cell_type":"code","execution_count":109,"metadata":{"id":"m7vBflpYQKof","trusted":true},"outputs":[],"source":["with open('./saves/dev_predictions_parser_only.txt', 'w') as f:\n","    for tree in predicted_dev_trees:\n","        f.write(' '.join(str(tree).split()) + '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-DeuFtciydn","trusted":true},"outputs":[],"source":["!data/EVALB/evalb -p data/EVALB/nk.prm dev saves/dev_predictions_parser_only.txt | tail -n 29"]},{"cell_type":"markdown","metadata":{"id":"p4phpKIGu4MW"},"source":["# Report: Parsing Error Analysis\n","\n","For this section, you will perform error analysis on your parser. You should aim for no more than 1-page worth of text, but may use more pages to fit any figures and visualizations you deem appropriate. You may use any editor you like, but we recommend using LaTeX and working in an environment like Overleaf. \n","\n","You are free to decide on what analysis to perform. Your analysis should dive into the details of an aspect of the model's behavior beyond just reporting its evaluation performance from EVALB. Some examples of what you could do: \n","\n","* Analyze precision-recall curves, training graphs, or other experimental statistics. \n","* Comparison of model performance across word categories or sentence types (e.g. NP vs VP), accompanied by appropriate statistics. \n","* A qualitative analysis of how your model resolves instances of structural ambiguity in sentences you write (https://allthingslinguistic.com/post/52411342274/how-many-meanings-can-you-get-for-the-sentence-i).\n","* etc.\n","\n","For full credit, your report should include the following: \n","1. An in depth analysis of your choice into an aspect of the model's parsing behavior. \n","2. Figures supporting your analysis, e.g. plots, tables, parse trees predicted by your model and ground-truth trees from the development set, etc. \n","\n","When you submit the report, name it **report.pdf**. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Your submission should consist of your code, as well as parsing and tagging predictions for both validation and test data.\n","\n","Here, the `predict` function runs both the tagger and the parser, combining their results into a single tree object. The predicted trees are saved as text files, with one line per tree."]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[],"source":["# Uncomment this code to restore models from a checkpoint.\n","#\n","# tagging_model = TransformerPOSTaggingModel().to(device)\n","# tagging_model.load_state_dict(torch.load('./saves/tagging_model.pt'))\n","# parsing_model = TransformerParsingModel().to(device)\n","# parsing_model.load_state_dict(torch.load('./saves/parsing_model.pt'))\n","\n","with open('./saves/dev_predictions_tagger_only.txt', 'w') as f:\n","    for tagged_sent in predict_tags(tagging_model, 'dev'):\n","        leaves = [nltk.tree.Tree(tag, [word]) for word, tag in tagged_sent]\n","        dummy_tree = nltk.tree.Tree('TOP', leaves)\n","        f.write(' '.join(str(dummy_tree).split()) + '\\n')\n","\n","with open('./saves/dev_spans.txt', 'w') as f:\n","    for n, chart in enumerate(predict_span_label_scores(parsing_model, 'dev')):\n","        chart = np.argmax(chart, -1)\n","        for i in range(chart.shape[0]):\n","            for j in range(i+1, chart.shape[1]):\n","                if chart[i, j] > 0:\n","                    f.write(f'{n} {i} {j} {SPAN_LABELS[chart[i, j]]}\\n')\n","\n","with open('./saves/dev_predictions_parser_only.txt', 'w') as f:\n","    for tree in predict(parsing_model, 'dev'):\n","        f.write(' '.join(str(tree).split()) + '\\n')\n","\n","with open('./saves/test_predictions.txt', 'w') as f:\n","    for tree in predict(parsing_model, 'test', tagging_model=tagging_model):\n","        f.write(' '.join(str(tree).split()) + '\\n')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For full credit on the tagging portion:\n","- `dev_tagging_accuracy` must exceed 95\n","- `test_tagging_accuracy` should exceed 94\n","\n","For full credit on the parsing portion:\n","- `dev_f1` must exceed 89\n","- `test_f1` should exceed 88\n","\n","For partial credit on the parsing portion, due to not having implemented CKY:\n","- `dev_f1_no_cky` should exceed 86"]},{"cell_type":"markdown","metadata":{"id":"kotdLszxHWWJ"},"source":["## Submission"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-onu93vgG2-U"},"source":["Turn in the following files on Gradescope:\n","* hw3.ipynb (this file; please rename to match)\n","* dev_predictions_tagger_only.txt\n","* dev_spans.txt\n","* dev_predictions_parser_only.txt\n","* test_predictions.txt\n","* report.pdf\n","\n","Be sure to check the output of the autograder after it runs.  It should confirm that no files are missing and that the output files have the correct format."]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.16 ('nlp')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"vscode":{"interpreter":{"hash":"154abf72fb8cc0db1aa0e7366557ff891bff86d6d75b7e5f2e68a066d591bfd7"}}},"nbformat":4,"nbformat_minor":4}
