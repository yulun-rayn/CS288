{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CeUOYktpXtOz"
      },
      "source": [
        "# Project 4: Finetuning Transformer Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m1Uv47jiNX"
      },
      "source": [
        "In this project, you will first learn how to use Huggingface's Transformers library to load large language models. Next, we will generate text from these models. Finally, we will finetune models on two tasks (sentiment analysis and machine translation).\n",
        "\n",
        "This project will be more open ended than the previous projects. We expect you to learn how to use the huggingface and torch documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acPh_4GwYID0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we install and import the required dependencies. These include:\n",
        "* `torch` for modeling and training\n",
        "* `transformers` for pre-trained models\n",
        "* `datasets` from huggingface to load existing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4LV8KY6_unfe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install --upgrade sacrebleu sentencepiece\n",
        "\n",
        "# Standard library imports\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM, GenerationConfig, EarlyStoppingCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNT_TURTwIlW"
      },
      "source": [
        "Before proceeding, let's verify that we're connected to a GPU runtime and that `torch` can detect the GPU.\n",
        "We'll define a variable `device` here to use throughout the code so that we can easily change to run on CPU for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYpIPtqtwVwh",
        "outputId": "e4132baa-6878-4fff-c6a4-3920b45ae0c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"Found GPU\")\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    print(\"Did not find GPU\")\n",
        "    device = \"cpu\"\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs('./saves/', exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANK-5cMtYSyH"
      },
      "source": [
        "### Loading Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We4sTUA5j0Ab"
      },
      "source": [
        "We will use GPT-2 medium for this project. This includes both the GPT-2 tokenizer and the GPT-2 model weights itself. If you want to learn more about this model, you can read the GPT-2 paper https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\n",
        "\n",
        "Let's first load the tokenizer for the GPT-2 medium model. You can find how to do this by reading the documentation for AutoTokenzier in transformers, and finding the GPT-2 model of ~345 million params in there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Gr9PbArCvCT0"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0rkjeg5bezU"
      },
      "source": [
        "Let's tokenize and detokenize some text from this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HALePz7Cbjzj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15496, 995]\n",
            "Hello world\n",
            "[39, 5708, 11, 269, 10205, 5908, 1556, 40138, 47249, 235]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.encode('Hello world'))\n",
        "print(tokenizer.decode(tokenizer.encode('Hello world')))\n",
        "print(tokenizer.encode(\"Hola, c√≥mo est√°süòç\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4aSFr-hmKpw"
      },
      "source": [
        "Now let's load the GPT-2 medium model. Make sure you also put the model onto the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W1owrfI1xjby"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (12): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (13): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (14): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (15): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (16): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (17): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (18): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (19): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (20): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (21): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (22): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (23): GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your code here\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
        "gpt2_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1aBbR3snOjV"
      },
      "source": [
        "## Generate From the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mo0tnIbnQz8"
      },
      "source": [
        "Now let's generate some text from the model to test its LM capabilities. Let's generate 10 pieces of random text of length 50 tokens from the model using random sampling with temperature set to 0.7. This will allow the text to be somewhat high in diversity (random sampling) while maintaining reasonable quality (temperature < 1). When generating text, you can condition on phrases such as \"The coolest thing in NLP right now is\". Find the relevant function and arguments to use for generating text using the Huggingface documentation.\n",
        "\n",
        "Hint: you may find https://huggingface.co/docs/transformers/main_classes/text_generation to be useful for learning about generating from LMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8xSUaso9vo1V"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\"<|startoftext|>The coolest thing right now in NLP is\", return_tensors=\"pt\").input_ids.to(device)\n",
        "# Your code here\n",
        "generation_config = GenerationConfig.from_pretrained(\n",
        "    \"gpt2-medium\", do_sample=True, num_return_sequences=10, max_new_tokens=50-len(inputs[0]), temperature=0.7, pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "sample_outputs = gpt2_model.generate(inputs, generation_config=generation_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uVROVwndt_z"
      },
      "source": [
        "Now lets print the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "koCYZxALdvjX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: <|startoftext|>The coolest thing right now in NLP is that you can have a lot of different words with different meanings depending on how the language is used. So if you're learning the language, then anything that is translated to\n",
            "1: <|startoftext|>The coolest thing right now in NLP is the one that allows you to \"fake\" human speech. What it's like to be a human being in this world. It's like you're in a movie and\n",
            "2: <|startoftext|>The coolest thing right now in NLP is that you can get a really really deep insight into the brain of someone who's been studying them, looking at their thoughts, or anything else. So, you can get\n",
            "3: <|startoftext|>The coolest thing right now in NLP is that the subject of your query is often an image or text. With this, you can query for every instance of the subject in the database while not using any query engine\n",
            "4: <|startoftext|>The coolest thing right now in NLP is the one-shot approach, where you just do one thing rather than having multiple steps. If you take an example from my last post, you could use a single-\n",
            "5: <|startoftext|>The coolest thing right now in NLP is the use of \"startoftext\" keywords, which are used to generate text before the end of the input. This allows you to do things like: \"select the\n",
            "6: <|startoftext|>The coolest thing right now in NLP is using the word 'what' to talk about 'what' in a very specific way. So if you wanna learn more about it, just do that. --|--\n",
            "7: <|startoftext|>The coolest thing right now in NLP is machine learning, which is a technology that's been around for a while, but has only recently arrived in the public good. This is a technique that allows you to learn\n",
            "8: <|startoftext|>The coolest thing right now in NLP is machine learning. You can use it to answer questions like, what does this image mean? But it can also be used to predict what an image might look like by looking\n",
            "9: <|startoftext|>The coolest thing right now in NLP is how easily you can get all the words together into a common format. In fact, NLP's wordlists are the best available, and they're incredibly easy to use\n"
          ]
        }
      ],
      "source": [
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ox_6NYWCoydJ"
      },
      "source": [
        "Now generate one piece of text of length 50 with the same prompt (\"The coolest thing right now in NLP is\") but use greedy decoding. This roughly corresponds to generating some text that is high likelihood for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EQEwpPUHePmC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|startoftext|>The coolest thing right now in NLP is the ability to use the word \"startoftext\" to refer to a word that is not part of the text. This is useful for things like \"the word\"'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(\"<|startoftext|>The coolest thing right now in NLP is\", return_tensors=\"pt\").input_ids.to(device)\n",
        "# Your code here\n",
        "generation_config = GenerationConfig.from_pretrained(\n",
        "    \"gpt2-medium\", max_new_tokens=50-len(inputs[0]), pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "outputs = gpt2_model.generate(inputs, generation_config=generation_config)[0]\n",
        "tokenizer.decode(outputs, skip_special_tokens=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjVBlj9yfPbx"
      },
      "source": [
        "Now let's try to see how good of a translation system GPT-2 medium is when used \"out of the box\". To accomplish this, we can condition on a prompt like the one below and generate from the model with greedy decoding. This will attempt to translate the sentence \"UC Berkeley ist eine Schule in Kalifornien\", which means \"UC Berkeley is a school in California\". Make sure to set the max length to be high enough so that the model generates sufficient text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Xj-OHYlppX4N"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Translate the following texts into English.\n",
        "\n",
        "German: UC Berkeley ist eine Schule in Kalifornien\n",
        "English:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VfqUVa8NfO2j"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|startoftext|>Translate the following texts into English.\\n\\nGerman: UC Berkeley ist eine Schule in Kalifornien\\nEnglish: UC Berkeley ist eine Schule in Kalifornien\\n\\nTranslate the following texts'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your code here. Generate from the model using greedy decoding with the above prompt\n",
        "inputs = tokenizer(\"<|startoftext|>\"+prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "generation_config = GenerationConfig.from_pretrained(\n",
        "    \"gpt2-medium\", max_new_tokens=20, pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "outputs = gpt2_model.generate(inputs, generation_config=generation_config)[0]\n",
        "tokenizer.decode(outputs, skip_special_tokens=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TBGuw-18qjsB"
      },
      "source": [
        "As we can see, translation quality is terrible, as it just repeats the words from the previous text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlBIoN4aragA"
      },
      "source": [
        "Now, let's finetune GPT-2 on the translation task to improve the results. We will use a translation dataset from the Huggingface dataset repository (it has thousands of other datasets available). This dataset is one of TED talks translated between German and English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SJYzMQxfvrr0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset ted_talks_iwslt (/home/ubuntu/.cache/huggingface/datasets/ted_talks_iwslt/de_en_2014-9408486716c87367/1.1.0/a42f763b98f8e9cc19358a2ac1007b0b600554e260ee48e6316df39703ef37a4)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67ef316c9320464dbe6047b60264631d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import datasets\n",
        "dataset = datasets.load_dataset(\"ted_talks_iwslt\", language_pair=(\"de\", \"en\"), year=\"2014\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OSvnbe8Bj8f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'de': '\"Ich habe Zerebralparese. Ich zappele die ganze Zeit\", k√ºndigt Maysoon Zayid zu Anfang dieses ungeheuer witzigen, erheiternden an. (Er ist wirklich ungeheur witzig.) \"Als w√ºrde Shakira auf Muhammad Ali treffen.\" Elegant und scharfsinnig nimmt uns die arabisch-amerikanische Komikerin auf eine Reise durch ihre Abenteuer als Schauspielerin, Komikerin, Philanthropin und F√ºrsprecherin f√ºr Menschen mit Behinderungen mit.', 'en': '\"I have cerebral palsy. I shake all the time,\" Maysoon Zayid announces at the beginning of this exhilarating, hilarious talk. (Really, it\\'s hilarious.) \"I\\'m like Shakira meets Muhammad Ali.\" With grace and wit, the Arab-American comedian takes us on a whistle-stop tour of her adventures as an actress, stand-up comic, philanthropist and advocate for the disabled.'}\n"
          ]
        }
      ],
      "source": [
        "print(dataset['train'][0]['translation'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISt5r0qKnGCg"
      },
      "source": [
        "Now we can create a dataset. For each element in the dataset, it should have a text prompt and then the translation, similar to above. Your job is to fill in the labels field below. This field sets the labels to use for training during the language modeling task. \n",
        "\n",
        "For the labels, we only want to train the model to output the text after the words \"English:\". This is because in the prompt, everything before the words \"English:\" will also be provided to the model as input. Hint: use -100 as the label for tokens you do not want to train on.\n",
        "Hint 2: When doing LM training, the labels are the same as the input tokens, except shifted to the left by one. You should check whether Huggingface is already doing the shifting, or whether you need to do the shifting yourself.\n",
        "\n",
        "One thing to be careful of with all LMs is to make sure there are not extra spaces. So, the text should be formatted as like \"English: Hello...\" not \"English:  Hello...\". This issue is a common problem people face when using APIs like GPT-3 which we will cover next time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZtBrXm2Ym4uy"
      },
      "outputs": [],
      "source": [
        "prompt1 = \"\"\"Translate the following texts into English.\n",
        "German:\"\"\"\n",
        "prompt2 = \"\"\"\n",
        "English:\"\"\"\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, examples, tokenizer):\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = []\n",
        "        for example in examples:\n",
        "            training_text = prompt1+\" \"+example['translation']['de']+prompt2+\" \"+example['translation']['en']+\"<|endoftext|>\"\n",
        "            encodings_dict = tokenizer(training_text, max_length=275, padding=\"max_length\", truncation=True)\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "            prompt_and_input_length = len(tokenizer.encode(prompt1+\" \"+example['translation']['de']+prompt2))\n",
        "            # your code below\n",
        "            self.labels.append(torch.cat(\n",
        "                (torch.full((prompt_and_input_length,), -100),\n",
        "                 torch.tensor(encodings_dict['input_ids'][prompt_and_input_length:]))\n",
        "            ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids':self.input_ids[idx], 'attention_mask':self.attn_masks[idx], 'labels':self.labels[idx]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rjfb7hkmrAze"
      },
      "outputs": [],
      "source": [
        "translation_dataset = TranslationDataset(dataset['train'], tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0xe93WarEOd"
      },
      "source": [
        "Now let's break the dataset into a train and test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1cV_oz5rpdOe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2674\n",
            "298\n"
          ]
        }
      ],
      "source": [
        "train_size = int(0.9 * len(translation_dataset))\n",
        "train_dataset, val_dataset = random_split(translation_dataset, [train_size, len(translation_dataset) - train_size])\n",
        "print(len(train_dataset))\n",
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ijr5Pn_uqk5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([ 8291, 17660,   262,  1708, 13399,   656,  3594,    13,   198, 16010,\n",
            "           25,  9626,  8687, 18320, 42803,  2688,  6877,   288,   292,   304,\n",
            "        10745,  4891, 11896, 46097, 45371, 23773,   920, 35875,    83,    11,\n",
            "          288,   562,  4656,   412,   328,   641, 11693, 14785, 18042,   520,\n",
            "        11033,    67,  1452,   842,  2120,    13,   370, 48988,  1481,    11,\n",
            "          509, 22157,   270, 11033,   912, 10366,   268,    11, 45371,   354,\n",
            "         7972,   328,   365,   270,  4587,   376,  1046,    70, 11033,   782,\n",
            "          263,  3318,   410,   494,   293,   290,   567,  1081,   431,    74,\n",
            "          660, 18042,   520, 11033,    67,  1452,   300,   562,   268,   264,\n",
            "          488,   257,   385,   304,  7274,   304,   259,    89,  9324,  1902,\n",
            "         9101,    82,   325,   607,   293,   270,   268,    25,  9626,  1355,\n",
            "           85,  9101,    75,  6122,  2150,    82,    89, 15668,    13,   554,\n",
            "        10564,   368,  6184,   120,   527,  8847,  2395,   358,   268,   569,\n",
            "          419, 22562, 18042, 38436, 22289, 41271,   328,    83,  1931,    11,\n",
            "          266,   494,   288,   292, 46212,  5378,    72,   861,  3318,   266,\n",
            "          494,  6184,    97, 21116, 33467,  4656, 45371,   316,  2736, 18042,\n",
            "         7221,  1042,   268,  3318, 31623,   268,   264,   521,    13,   198,\n",
            "        15823,    25,  8687, 48187, 42803,  2688,   468,  1043,   326,  2829,\n",
            "           11, 18069,  3657,  1089,   262,  6608,   286,  4736,  1377,   326,\n",
            "         5129,    11,  4065,  2494,    11,  6155,  2866,   290,   867,   584,\n",
            "         7612,   286,   257,  1748,   460,   307,  4648, 19513,   422,   257,\n",
            "         2060,  1271,    25,   262,  1748,   338,  3265,    13,   554,   428,\n",
            "         2000,    12, 49667,  1561,   422, 38436, 22289,   339,  2523,   703,\n",
            "          340,  2499,   290,   703,  2092,  3657,  1745,   329, 20296,   290,\n",
            "        10225,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  8687, 48187, 42803,  2688,   468,  1043,   326,  2829,\n",
            "           11, 18069,  3657,  1089,   262,  6608,   286,  4736,  1377,   326,\n",
            "         5129,    11,  4065,  2494,    11,  6155,  2866,   290,   867,   584,\n",
            "         7612,   286,   257,  1748,   460,   307,  4648, 19513,   422,   257,\n",
            "         2060,  1271,    25,   262,  1748,   338,  3265,    13,   554,   428,\n",
            "         2000,    12, 49667,  1561,   422, 38436, 22289,   339,  2523,   703,\n",
            "          340,  2499,   290,   703,  2092,  3657,  1745,   329, 20296,   290,\n",
            "        10225,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256])}\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1sLfI6vDriWK"
      },
      "source": [
        "Now we can use the Huggingface Trainer to finetune GPT-2 on this dataset. This abstracts away all of the details of training. Setup the training arguments to perform 3 epochs of training on this dataset, use a per-device batch size of 2 with gradient accumulation set to 8, use 100 warmup steps, a weight decay of 0.05. Set the eval batch size to be 2. Save a checkpoint every epoch. Set fp16 to True. Save the checkpoint in a specific output_dir so you can load it later. Hint: if it tries to launch Wandb, you may add the argument report_to=\"none\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eC4S6WBCsjOW"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "training_args = TrainingArguments(output_dir=\"./saves/translation_trainer\", num_train_epochs=3, optim=\"adamw_torch\",\n",
        "    per_device_train_batch_size=2, gradient_accumulation_steps=8, warmup_steps=100, weight_decay=0.05,\n",
        "    per_device_eval_batch_size=2, evaluation_strategy=\"epoch\", save_strategy=\"epoch\", save_steps=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsxRb6TOVHn5"
      },
      "source": [
        "Next create a Huggingface Trainer object and call train() on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xheKi30BVVJC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  3/501 00:03 < 27:35, 0.30 it/s, Epoch 0.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=501, training_loss=0.7546390847650593, metrics={'train_runtime': 1899.9537, 'train_samples_per_second': 4.222, 'train_steps_per_second': 0.264, 'total_flos': 3998491818393600.0, 'train_loss': 0.7546390847650593, 'epoch': 3.0})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your code here\n",
        "trainer = Trainer(\n",
        "    model=gpt2_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnbquLGFx0Y2"
      },
      "source": [
        "Now load your saved checkpoint and see how well the finetuned GPT-2 model does on translating the sentence from before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3IR7l2P_L9ey"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Translate the following texts into English.\\n\\nGerman: UC Berkeley ist eine Schule in Kalifornien\\nEnglish: UC Berkeley: A school in Kalifornia'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# your code here\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "generation_config = GenerationConfig.from_pretrained(\n",
        "    \"gpt2-medium\", max_new_tokens=20, pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "outputs = gpt2_model.generate(inputs, generation_config=generation_config)[0]\n",
        "tokenizer.decode(outputs, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUCG_xSYN2pn"
      },
      "source": [
        "If training went correctly, you should see a reasonable translation of the sentence, with some errors.\n",
        "\n",
        "For the project report, find two sentences where the model succeeds and two sentences where the model fails. Describe what might be causing these types of failures."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qC_sFmejXEtA"
      },
      "source": [
        "Finally, revisit the code from project 2 on using and running the Multi30k dataset. Your goal will be to translate the test set using the GPT-2 model you just finetuned. You will then submit your test predictions as a txt file, where you place your model's prediction for each test example on a separate line. Feel free to copy and paste any code from HW2 that may be useful. Submit the file named as mt_predictions.txt to gradescope.\n",
        "\n",
        "The GPT-2 model may not work that well on the Multi30k dataset, because there is a distribution shift where the Multi30k data looks different than the Ted talks data that you finetuned the model on. The takeaway is that a general-purpose LM system can be decent at a task like translation, however, if you create a domain-specific model like a LSTM trained specifically on Multi30k, you can outperform the general purpose model.\n",
        "\n",
        "For the project report, compare two translations from the GPT-2 versus LSTM model. Which one works better?\n",
        "\n",
        "Hint: One failure mode for GPT-2 is that it may generate fluent sentences that are actually unrelated to the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt1 = \"\"\"Translate the following texts into English.\n",
        "German:\"\"\"\n",
        "prompt2 = \"\"\"\n",
        "English:\"\"\"\n",
        "\n",
        "def get_raw_predictions(gpt_model, dataset, tokenizer, method, batch_size=64):\n",
        "    source_sentences = [prompt1+\" \"+example.src+prompt2 for example in dataset]\n",
        "    if method == \"greedy\":\n",
        "        generation_config = GenerationConfig.from_pretrained(\n",
        "            \"gpt2-medium\", max_new_tokens=100, num_beams=5, pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "    else:\n",
        "        generation_config = GenerationConfig.from_pretrained(\n",
        "            \"gpt2-medium\", max_new_tokens=100, num_beams=1, pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    padding_side = tokenizer.padding_side\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    predictions = []\n",
        "    for start_index in range(0, len(source_sentences), batch_size):\n",
        "        inputs = tokenizer(source_sentences[start_index:start_index + batch_size],\n",
        "            padding=True, return_tensors=\"pt\")\n",
        "\n",
        "        outputs = gpt_model.generate(\n",
        "            input_ids=inputs[\"input_ids\"].to(device),\n",
        "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "            generation_config=generation_config)\n",
        "\n",
        "        prediction_batch = tokenizer.batch_decode(\n",
        "            outputs[:, inputs[\"input_ids\"].shape[-1]:],\n",
        "            skip_special_tokens=True)\n",
        "        predictions.extend([prediction.lstrip(\" \") for prediction in prediction_batch])\n",
        "    tokenizer.padding_side = padding_side\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hRws9idEXecL"
      },
      "outputs": [],
      "source": [
        "# Your code for generating mt_predictions.txt below\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "\n",
        "extensions = [\".de\", \".en\"]\n",
        "source_field = data.Field(tokenize=lambda x: x)\n",
        "target_field = data.Field(tokenize=lambda x: x)\n",
        "_, _, test_data = datasets.Multi30k.splits(\n",
        "    extensions, [source_field, target_field], root=\".\")\n",
        "\n",
        "predictions = get_raw_predictions(gpt2_model, test_data, tokenizer, \"beam\", batch_size=64)\n",
        "with open('./saves/mt_predictions.txt', 'w') as outfile:\n",
        "    for prediction in predictions:\n",
        "        outfile.write(f\"{prediction}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0WsHbgnOAFx"
      },
      "source": [
        "### Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VmRQfoyOA2G"
      },
      "source": [
        "The beauty of language models is that we can apply this exact same machinery to solve a completely different task of sentiment analysis. Here, we will be given a movie review and the goal is to have the model predict whether the review is positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhJ5ptLyOoGI"
      },
      "source": [
        "First, we will load some sentiment analysis data. Your job is to copy what we did above for machine translation to load the dataset, build a Class to create the dataset, etc., \n",
        "\n",
        "When doing so, use the prompt below, where you put the text of the input in the first [] and in the second [], put the word Positive if the label is 1 and the word Negative if the label is 0. Make sure to also set the self.labels field correctly, we only want to compute a loss on the words Positive/Negative, and no other tokens in the model's input.\n",
        "\n",
        "The following is a movie review. [Movie Review Text Here]. The sentiment of the review is [Positive/Negative]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Qk67kbfPQAGy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset sst2 (/home/ubuntu/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "347d0aeac0f445b0b49c7cb604a81eae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import datasets\n",
        "dataset = datasets.load_dataset(\"sst2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ0CbUm8L0ZR"
      },
      "source": [
        "Note: Some people were saying that this line of code wasn't working and they needed to use \"dataset = datasets.load_dataset('glue', 'sst2')\" instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "j7_GzIgmRIGC"
      },
      "outputs": [],
      "source": [
        "prompt1 = \"\"\"The following is a movie review.\"\"\"\n",
        "prompt2 = \"\"\"\n",
        "The sentiment of the review is\"\"\"\n",
        "\n",
        "sentiments = [\"Negative\", \"Positive\"]\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    # Your code below\n",
        "    def __init__(self, examples, tokenizer):\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = []\n",
        "        for example in examples:\n",
        "            training_text = prompt1+\" \"+example['sentence']+prompt2+\" \"+sentiments[example['label']]+\"<|endoftext|>\"\n",
        "            encodings_dict = tokenizer(training_text, max_length=275, padding=\"max_length\", truncation=True)\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "            prompt_and_input_length = len(tokenizer.encode(prompt1+\" \"+example['sentence']+prompt2))\n",
        "            self.labels.append(torch.cat(\n",
        "                (torch.full((prompt_and_input_length,), -100),\n",
        "                 torch.tensor(encodings_dict['input_ids'][prompt_and_input_length:]))\n",
        "            ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids':self.input_ids[idx], 'attention_mask':self.attn_masks[idx], 'labels':self.labels[idx]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2lsk5uGNRIGE"
      },
      "outputs": [],
      "source": [
        "sentiment_train_dataset = SentimentDataset(dataset['train'], tokenizer)\n",
        "sentiment_val_dataset = SentimentDataset(dataset['validation'], tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUye-UIRRIGF"
      },
      "source": [
        "The data already comes with a validation and train split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Kb0XeLriRIGF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67349\n",
            "872\n"
          ]
        }
      ],
      "source": [
        "print(len(sentiment_train_dataset))\n",
        "print(len(sentiment_val_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7vOS-ZcTHds"
      },
      "source": [
        "Now let's train the model using the same trainer arguments as before, except just do $<$1 epoch of training because this dataset is quite large and training on the entire thing will take some time. Make sure you also use a different output_dir so it doesn't overwrite your old results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vEAs8zFDVdY4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2850' max='4209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2850/4209 3:56:09 < 1:52:41, 0.20 it/s, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.003866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.001307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.001018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.001210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.001230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.001142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.001354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.360500</td>\n",
              "      <td>0.000980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.360500</td>\n",
              "      <td>0.001007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.360500</td>\n",
              "      <td>0.001171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.360500</td>\n",
              "      <td>0.000970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.360500</td>\n",
              "      <td>0.000958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.360500</td>\n",
              "      <td>0.000815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.360500</td>\n",
              "      <td>0.001019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.360500</td>\n",
              "      <td>0.000846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.360500</td>\n",
              "      <td>0.000906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.360500</td>\n",
              "      <td>0.000942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000805</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2850, training_loss=0.06385934582927771, metrics={'train_runtime': 14173.247, 'train_samples_per_second': 4.752, 'train_steps_per_second': 0.297, 'total_flos': 2.274591154176e+16, 'train_loss': 0.06385934582927771, 'epoch': 0.68})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your code here\n",
        "training_args = TrainingArguments(output_dir=\"./saves/sentiment_trainer\", num_train_epochs=1, optim=\"adamw_torch\",\n",
        "    per_device_train_batch_size=2, gradient_accumulation_steps=8, warmup_steps=100, weight_decay=0.05,\n",
        "    per_device_eval_batch_size=2, evaluation_strategy=\"steps\", eval_steps=50, save_total_limit=5, load_best_model_at_end=True)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=gpt2_model,\n",
        "    args=training_args,\n",
        "    train_dataset=sentiment_train_dataset,\n",
        "    eval_dataset=sentiment_val_dataset,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azaXJt4cPV3Y"
      },
      "source": [
        "At test-time, when you want to classify an incoming movie review, you can just check whether the model generates the words Positive or Negative as the final word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wlTivogUUyFz"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"The following is a movie review. The acting was great but overall I was left disappointed by the film.\n",
        "The sentiment of the review is\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "l3-2HzbHVJ9L"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The following is a movie review. The acting was great but overall I was left disappointed by the film.\\nThe sentiment of the review is Negative'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your code here\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "generation_config = GenerationConfig.from_pretrained(\n",
        "    \"gpt2-medium\", max_new_tokens=20, pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "outputs = gpt2_model.generate(inputs, generation_config=generation_config)[0]\n",
        "tokenizer.decode(outputs, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc5bgcS7X1BX"
      },
      "source": [
        "Finally, run the entire validation set through the model and get your model predictions. Save the results as a txt file, where each line just contains either \"1\" if your model predicted Positive and \"0\" if the model predicted Negative. You will get full credit if your model's accuracy is greater than 80%. Save the file as sst_predictions.txt and submit it to gradescope.\n",
        "\n",
        "For the report, describe two possible improvements to your sentiment classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt1 = \"\"\"The following is a movie review.\"\"\"\n",
        "prompt2 = \"\"\"\n",
        "The sentiment of the review is\"\"\"\n",
        "\n",
        "def get_raw_predictions(gpt_model, dataset, tokenizer, method, batch_size=64):\n",
        "    source_sentences = [prompt1+\" \"+example['sentence']+prompt2 for example in dataset]\n",
        "    if method == \"greedy\":\n",
        "        generation_config = GenerationConfig.from_pretrained(\n",
        "            \"gpt2-medium\", max_new_tokens=100, num_beams=5, pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "    else:\n",
        "        generation_config = GenerationConfig.from_pretrained(\n",
        "            \"gpt2-medium\", max_new_tokens=100, num_beams=1, pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    padding_side = tokenizer.padding_side\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    predictions = []\n",
        "    for start_index in range(0, len(source_sentences), batch_size):\n",
        "        inputs = tokenizer(source_sentences[start_index:start_index + batch_size],\n",
        "            padding=True, return_tensors=\"pt\")\n",
        "\n",
        "        outputs = gpt_model.generate(\n",
        "            input_ids=inputs[\"input_ids\"].to(device),\n",
        "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "            generation_config=generation_config)\n",
        "\n",
        "        prediction_batch = tokenizer.batch_decode(\n",
        "            outputs[:, inputs[\"input_ids\"].shape[-1]:],\n",
        "            skip_special_tokens=True)\n",
        "        predictions.extend([prediction.lstrip(\" \") for prediction in prediction_batch])\n",
        "    tokenizer.padding_side = padding_side\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wnmzEl7wYGW2"
      },
      "outputs": [],
      "source": [
        "# Your code here for generating sst_predictions\n",
        "predictions = get_raw_predictions(gpt2_model, dataset['validation'], tokenizer, \"beam\", batch_size=64)\n",
        "with open('./saves/sst_predictions.txt', 'w') as outfile:\n",
        "    for prediction in predictions:\n",
        "        if prediction == \"Positive\" or prediction == \"positive\":\n",
        "            outfile.write(f\"1\\n\")\n",
        "        elif prediction == \"Negative\" or prediction == \"negative\":\n",
        "            outfile.write(f\"0\\n\")\n",
        "        else:\n",
        "            outfile.write(f\"-1\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kotdLszxHWWJ"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-onu93vgG2-U"
      },
      "source": [
        "Turn in the following files on Gradescope:\n",
        "* hw4.ipynb (this file; please rename to match)\n",
        "* mt_predictions.txt (the predictions for the Multi30k test set)\n",
        "* sst_predictions.txt (the predictions for the SST-2 validation set)\n",
        "* report.pdf\n",
        "\n",
        "Be sure to check the output of the autograder after it runs.  It should confirm that no files are missing and that the output files have the correct format."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "154abf72fb8cc0db1aa0e7366557ff891bff86d6d75b7e5f2e68a066d591bfd7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
