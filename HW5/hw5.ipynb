{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Project 5: Prompting With Large Language Models"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this project, we learn how to solve tasks by prompting existing LLM APIs. We will experiment with zero-shot and few-shot prompting and different methods for example selection for a semantic parsing task."]},{"cell_type":"markdown","metadata":{},"source":["First we install and import the required dependencies. These include:\n","* `openai` as our API for querying LLMs (you are free to choose to use a different LLM API if you would like)\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%%capture\n","%pip install openai"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["If you are using the OpenAI API, then go to create an account and then copy your secret API key from `https://platform.openai.com/account/api-keys`. Set this as an environment variable or key management service so we can load it below. Make sure to keep private key secret. You may use a different LLM service eg. Cohere (https://cohere.ai/)."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"]},{"cell_type":"markdown","metadata":{},"source":["If you have successfully authorized, then you should be able to see a list of available models by running the command below."]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"data":{"text/plain":["['babbage',\n"," 'davinci',\n"," 'text-davinci-edit-001',\n"," 'gpt-3.5-turbo-0301',\n"," 'babbage-code-search-code',\n"," 'text-similarity-babbage-001',\n"," 'gpt-3.5-turbo',\n"," 'code-davinci-edit-001',\n"," 'text-davinci-001',\n"," 'text-davinci-003',\n"," 'ada',\n"," 'babbage-code-search-text',\n"," 'babbage-similarity',\n"," 'code-search-babbage-text-001',\n"," 'text-curie-001',\n"," 'whisper-1',\n"," 'code-search-babbage-code-001',\n"," 'text-ada-001',\n"," 'text-embedding-ada-002',\n"," 'text-similarity-ada-001',\n"," 'curie-instruct-beta',\n"," 'ada-code-search-code',\n"," 'ada-similarity',\n"," 'code-search-ada-text-001',\n"," 'text-search-ada-query-001',\n"," 'davinci-search-document',\n"," 'ada-code-search-text',\n"," 'text-search-ada-doc-001',\n"," 'davinci-instruct-beta',\n"," 'text-similarity-curie-001',\n"," 'code-search-ada-code-001',\n"," 'ada-search-query',\n"," 'text-search-davinci-query-001',\n"," 'curie-search-query',\n"," 'davinci-search-query',\n"," 'babbage-search-document',\n"," 'ada-search-document',\n"," 'text-search-curie-query-001',\n"," 'text-search-babbage-doc-001',\n"," 'curie-search-document',\n"," 'text-search-curie-doc-001',\n"," 'babbage-search-query',\n"," 'text-babbage-001',\n"," 'text-search-davinci-doc-001',\n"," 'text-search-babbage-query-001',\n"," 'curie-similarity',\n"," 'curie',\n"," 'text-similarity-davinci-001',\n"," 'text-davinci-002',\n"," 'davinci-similarity',\n"," 'cushman:2020-05-03',\n"," 'ada:2020-05-03',\n"," 'babbage:2020-05-03',\n"," 'curie:2020-05-03',\n"," 'davinci:2020-05-03',\n"," 'if-davinci-v2',\n"," 'if-curie-v2',\n"," 'if-davinci:3.0.0',\n"," 'davinci-if:3.0.0',\n"," 'davinci-instruct-beta:2.0.0',\n"," 'text-ada:001',\n"," 'text-davinci:001',\n"," 'text-curie:001',\n"," 'text-babbage:001']"]},"execution_count":96,"metadata":{},"output_type":"execute_result"}],"source":["import openai\n","openai.api_key = OPENAI_API_KEY\n","[model[\"root\"] for model in openai.Model.list()[\"data\"]]"]},{"cell_type":"markdown","metadata":{},"source":["We will now evaluate the LLM on a semantic parsing task. Geoquery is a dataset that contains information about the geography of the United States. For more information, please see: https://www.cs.utexas.edu/users/ml/nldata/geoquery.html. We will experiment with the compositional split introduced in (Keysers et al., 2020) https://openreview.net/forum?id=SygcCnNKwr. First, let's download the train and validation data. The goal for the LLM is to take in English queries about US geography about population, elevation, etc. and output a formal representation of the query."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["os.makedirs('./saves/', exist_ok=True)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-04-22 20:22:43--  https://github.com/kl2806/geoquery/raw/main/data.zip\n","Resolving github.com (github.com)... 192.30.255.113\n","Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/kl2806/geoquery/main/data.zip [following]\n","--2023-04-22 20:22:43--  https://raw.githubusercontent.com/kl2806/geoquery/main/data.zip\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4493 (4.4K) [application/zip]\n","Saving to: ‘data/data.zip.2’\n","\n","data.zip.2          100%[===================>]   4.39K  --.-KB/s    in 0s      \n","\n","2023-04-22 20:22:43 (49.8 MB/s) - ‘data/data.zip.2’ saved [4493/4493]\n","\n","Archive:  data/data.zip\n","  inflating: data/train.tsv          \n","  inflating: data/dev.tsv            \n"]}],"source":["!wget https://github.com/kl2806/geoquery/raw/main/data.zip -P data/\n","!unzip -o data/data.zip -d data/"]},{"cell_type":"markdown","metadata":{},"source":["Let's take a look at the data, each instance should compose of an English utterance, and a formal representation of the utterance"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["how tall is the highest point in m0\tanswer ( elevation_1 ( highest ( intersection ( place , loc_2 ( m0 ) ) ) ) )\n","what is the largest city in m0\tanswer ( largest ( intersection ( city , loc_2 ( m0 ) ) ) )\n","what states border states that the m0 runs through\tanswer ( intersection ( state , next_to_2 ( intersection ( state , traverse_1 ( m0 ) ) ) ) )\n","what is the maximum elevation of m0\tanswer ( highest ( intersection ( place , loc_2 ( m0 ) ) ) )\n","what is the population of m0\tanswer ( population_1 ( m0 ) )\n"]}],"source":["!head -n 5 data/train.tsv"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num training examples: 440\n","Num dev examples: 40\n"]}],"source":["import csv\n","from dataclasses import dataclass\n","from typing import List\n","\n","@dataclass\n","class Example:\n","     query: str\n","     program: str\n","\n","training_examples: List[Example] = []\n","\n","with open('./data/train.tsv', 'r') as tsv_file:\n","    reader = csv.reader(tsv_file, delimiter='\\t')\n","    for query, program in reader:\n","        training_examples.append(Example(query, program))\n","\n","dev_examples: List[Example] = []\n","with open('./data/dev.tsv', 'r') as tsv_file:\n","    reader = csv.reader(tsv_file, delimiter='\\t')\n","    for query, program in reader:\n","        dev_examples.append(Example(query, program))\n","\n","print(f\"Num training examples: {len(training_examples)}\")\n","print(f\"Num dev examples: {len(dev_examples)}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, let's define a function that uses the OpenAI API to output the semantic parse. As a first cut, let's just try to describe the task in English and return it in `get_static_prompt`. Then implement `parse_example`, which should call the LLM API, and return the semantic parse. `gpt-3.5-turbo`, the corresponding API call to ChatGPT should work well for this task."]},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["filter(same_entity(m0, river), river)\n"]}],"source":["from typing import Callable\n","\n","def get_static_prompt(utterance: str):\n","    \"\"\"Return a prompt that doesn't change between different examples\"\"\"\n","    \"\"\"YOUR CODE HERE\"\"\"\n","    return [\n","        {\"role\": \"system\", \"content\": \"You are a semantic parser. Answer as concisely as possible.\"},\n","        {\"role\": \"user\", \"content\": \"Map the following query into functional query language: \" + utterance}\n","    ]\n","\n","def parse_example(model: str, utterance: str, prompt_method: Callable[[str], str], **kwargs: dict) -> str:\n","    \"\"\"Return the semantic parse of the utterance\"\"\"\n","    prompt = prompt_method(utterance, **kwargs)\n","    \"\"\"YOUR CODE HERE\"\"\"\n","    response = openai.ChatCompletion.create(model=model, messages=prompt)\n","    \n","    return response[\"choices\"][0][\"message\"][\"content\"]\n","\n","parse = parse_example(model=\"gpt-3.5-turbo\",\n","                      utterance=\"what river runs through m0\",\n","                      prompt_method=get_static_prompt)\n","\n","print(parse)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["With just an English description, the output probably does not look very similar to the target language that we want. Let's try to construct a prompt with some examples from our training set and see how it does. Implement the function below to uniformly sample examples from the training set, and use them to construct a few-shot prompt to the model."]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Uniform sampling prompt:\n","{'role': 'system', 'content': 'You are a semantic parser. Answer as concisely as possible.'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: number of people in m0'}\n","{'role': 'assistant', 'content': 'answer ( population_1 ( m0 ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: how many people live in m0'}\n","{'role': 'assistant', 'content': 'answer ( population_1 ( m0 ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what are the major cities in the state of m0'}\n","{'role': 'assistant', 'content': 'answer ( intersection ( major , intersection ( city , loc_2 ( intersection ( state , m0 ) ) ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: which of the states bordering m0 has the largest population'}\n","{'role': 'assistant', 'content': 'answer ( largest_one ( population_1 , intersection ( state , next_to_2 ( m0 ) ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: how many cities are in m0'}\n","{'role': 'assistant', 'content': 'answer ( count ( intersection ( city , loc_2 ( m0 ) ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: how many states are there in m0'}\n","{'role': 'assistant', 'content': 'answer ( count ( intersection ( state , loc_2 ( m0 ) ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: where is m0 located'}\n","{'role': 'assistant', 'content': 'answer ( loc_1 ( m0 ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what is the population in m0'}\n","{'role': 'assistant', 'content': 'answer ( population_1 ( m0 ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what rivers are in m0'}\n","{'role': 'assistant', 'content': 'answer ( intersection ( river , loc_2 ( m0 ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what is the total area of the m0'}\n","{'role': 'assistant', 'content': 'answer ( area_1 ( m0 ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what river runs through m0'}\n"]}],"source":["import random\n","from typing import List\n","\n","def get_prompt(utterance: str, examples: List[Example]):\n","    messages = [{\"role\": \"system\", \"content\": \"You are a semantic parser. Answer as concisely as possible.\"}]\n","\n","    for example in examples:\n","        messages.extend([\n","            {\"role\": \"user\", \"content\": \"Map the following query into functional query language: \" + example.query},\n","            {\"role\": \"assistant\", \"content\": example.program}\n","        ])\n","\n","    return messages + [\n","        {\"role\": \"user\", \"content\": \"Map the following query into functional query language: \" + utterance}\n","    ]\n","\n","def random_sample_prompt(utterance: str, training_examples: List[Example], num_samples: int = 10):\n","    \"\"\"Return a prompt for a given example\"\"\"\n","    \"\"\"YOUR CODE HERE\"\"\"\n","    examples = random.sample(training_examples, num_samples)\n","\n","    return get_prompt(utterance, examples)\n","\n","prompt = random_sample_prompt(utterance=\"what river runs through m0\",\n","                              training_examples=training_examples)\n","print(\"Uniform sampling prompt:\")\n","for entry in prompt:\n","    print(entry)"]},{"cell_type":"code","execution_count":131,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["answer ( intersection ( river , traverse_2 ( m0 ) ) )\n"]}],"source":["parse = parse_example(model=\"gpt-3.5-turbo\",\n","                      utterance=\"what river runs through m0\",\n","                      prompt_method=random_sample_prompt,\n","                      training_examples=training_examples)\n","print(parse)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, let's evaluate our uniform sampling prompt on the validation set. If you run into rate limit issues with the API, you may want to use a backoff strategy or consult one of the solutions here https://platform.openai.com/docs/guides/rate-limits/error-mitigation."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["%%capture\n","!pip install tenacity"]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[],"source":["import tqdm\n","from tenacity import (\n","    retry,\n","    wait_fixed,\n","    stop_after_attempt,\n",")\n","\n","REQUEST_PER_MIN = 3\n","\n","@retry(wait=wait_fixed(60/REQUEST_PER_MIN+1), stop=stop_after_attempt(100))\n","def completion_with_backoff(*args, **kwargs):\n","    return parse_example(*args, **kwargs)\n","\n","def get_predictions(model: str,\n","                    evaluation_examples: List[Example],\n","                    prompt_creation_function: Callable[[str], str],\n","                    **kwargs: List[str]) -> List[str]:\n","    \"\"\"Get a list of predictions from the evaluation examples\"\"\"\n","    predictions = []\n","    for example in tqdm.tqdm(evaluation_examples):\n","        predicted_program = completion_with_backoff(model, example.query, prompt_creation_function, **kwargs)\n","        predictions.append(predicted_program)\n","    return predictions\n","\n","def evaluate(predictions: List[str], evaluation_examples: List[Example]) -> float:\n","    \"\"\"Evaluate the accuracy of the predictions\"\"\"\n","    correct = 0\n","    for prediction, example in zip(predictions, evaluation_examples):\n","        if prediction == example.program:\n","            correct += 1\n","    return correct / len(evaluation_examples)"]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 40/40 [12:51<00:00, 19.28s/it]\n"]}],"source":["random_sample_predictions = get_predictions(model=\"gpt-3.5-turbo\",\n","                                            evaluation_examples=dev_examples,\n","                                            prompt_creation_function=random_sample_prompt,\n","                                            training_examples=training_examples)"]},{"cell_type":"markdown","metadata":{},"source":["The model should get at least 15\\% exact match with randomly sampling examples."]},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[],"source":["import re\n","\n","def save_predictions(predictions: List[str], file_path: str):\n","    with open(file_path, 'w') as f:\n","        for prediction in predictions:\n","            f.write(re.sub(\"\\n\", \" \", prediction))\n","            f.write('\\n')"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exact match for uniform sampling prompt: 0.3\n"]}],"source":["# Save the predictions as `random_predictions.txt`\n","save_predictions(random_sample_predictions, './saves/random_predictions.txt')\n","\n","exact_match = evaluate(random_sample_predictions, dev_examples)\n","print(f\"Exact match for uniform sampling prompt: {exact_match}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Randomly sampling examples does not consider the utterance when selecting examples. Next, we will try to pick examples for the prompt based on embedding similarity. First, let's install `sentence-transformers`, which we will use to get embeddings of the utterance."]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["%%capture\n","%pip install sentence-transformers"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, let's construct embeddings using a small pretrained model for each example in our training data. With the embeddings of all the training data, we can construct a function that takes in an utterance and outputs a prompt with examples having highest cosine similarity with the utterance."]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def get_nearest_neighbor_prompt(utterance: str,\n","                                training_examples: List[Example],\n","                                embedding_model: str,\n","                                num_samples: int = 10):\n","    model = SentenceTransformer(embedding_model)\n","    \"\"\"YOUR CODE HERE\"\"\"\n","    utterance_embedding = model.encode([utterance])\n","    corpus_embeddings = model.encode([example.query for example in training_examples])\n","\n","    similarities = cosine_similarity(utterance_embedding, corpus_embeddings)[0]\n","    indices = similarities.argsort()[::-1][:num_samples]\n","    examples = [training_examples[i] for i in indices]\n","\n","    return get_prompt(utterance, examples)"]},{"cell_type":"code","execution_count":144,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Similarity sampling prompt:\n","{'role': 'system', 'content': 'You are a semantic parser. Answer as concisely as possible.'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: which rivers are in m0'}\n","{'role': 'assistant', 'content': 'answer ( intersection ( river , loc_2 ( m0 ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what rivers are in m0'}\n","{'role': 'assistant', 'content': 'answer ( intersection ( river , loc_2 ( m0 ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what rivers are in m0'}\n","{'role': 'assistant', 'content': 'answer ( intersection ( river , loc_2 ( m0 ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what rivers are in m0'}\n","{'role': 'assistant', 'content': 'answer ( intersection ( river , loc_2 ( m0 ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what rivers are in m0'}\n","{'role': 'assistant', 'content': 'answer ( intersection ( river , loc_2 ( m0 ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: how many rivers are in m0'}\n","{'role': 'assistant', 'content': 'answer ( count ( intersection ( river , loc_2 ( m0 ) ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: how many rivers are in m0'}\n","{'role': 'assistant', 'content': 'answer ( count ( intersection ( river , loc_2 ( m0 ) ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: how many rivers are in m0'}\n","{'role': 'assistant', 'content': 'answer ( count ( intersection ( river , loc_2 ( m0 ) ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what are all the rivers in m0'}\n","{'role': 'assistant', 'content': 'answer ( intersection ( river , loc_2 ( m0 ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: how many rivers are there in m0'}\n","{'role': 'assistant', 'content': 'answer ( count ( intersection ( river , loc_2 ( m0 ) ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what river runs through m0'}\n"]}],"source":["prompt = get_nearest_neighbor_prompt(utterance=\"what river runs through m0\",\n","                           training_examples=training_examples,\n","                           embedding_model='all-MiniLM-L6-v2')\n","print(\"Similarity sampling prompt:\")\n","for entry in prompt:\n","    print(entry)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Evaluate the similarity based prompt on the validation data and save your predictions as `similarity_predictions.txt` with one prediction per line of the validation set. With similarity based example selection, we should get at least 20\\% exact match on the validation set. Note that there could be some duplicates in the training data because we are working with a version of the data where the location names are normalized to be variables like `m0`."]},{"cell_type":"code","execution_count":138,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 40/40 [13:08<00:00, 19.71s/it]\n"]}],"source":["similarity_predictions = get_predictions(model=\"gpt-3.5-turbo\",\n","                                         evaluation_examples=dev_examples,\n","                                         prompt_creation_function=get_nearest_neighbor_prompt,\n","                                         training_examples=training_examples,\n","                                         embedding_model='all-MiniLM-L6-v2')"]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exact match for nearest neighbor prompt: 0.7\n"]}],"source":["# Save the predictions as `similarity_predictions.txt`\n","save_predictions(similarity_predictions, './saves/similarity_predictions.txt')\n","\n","exact_match = evaluate(similarity_predictions, dev_examples)\n","print(f\"Exact match for nearest neighbor prompt: {exact_match}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's try to improve on nearest neighbor based example search. This part will be more open ended. We will now implement a different example selection method that improves over uniform random selection. You may implement an algorithm `Diverse Demonstrations Improve In-context Compositional Generalization` (https://arxiv.org/abs/2212.06800) or come up with your own example selection method. In the report, describe the algorithm that you implemented and intuition of why it could be effective."]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.cluster import KMeans\n","\n","def construct_diversity_prompt(utterance: str,\n","                               training_examples: List[Example],\n","                               embedding_model: str,\n","                               num_samples: int = 10,\n","                               random_state: int = 0):\n","    \"\"\"YOUR CODE HERE\"\"\"\n","    model = SentenceTransformer(embedding_model)\n","    utterance_embedding = model.encode([utterance])\n","    corpus_embeddings = model.encode([example.query for example in training_examples])\n","\n","    similarities = cosine_similarity(utterance_embedding, corpus_embeddings)[0]\n","\n","    kmeans = KMeans(\n","        n_clusters=num_samples, random_state=random_state, n_init=\"auto\"\n","    ).fit(corpus_embeddings)\n","    examples = []\n","    for label in np.unique(kmeans.labels_):\n","        cluster_indices = np.where(kmeans.labels_ == label)[0]\n","        index = cluster_indices[similarities[cluster_indices].argsort()[-1]]\n","        examples.append(training_examples[index])\n","\n","    return get_prompt(utterance, examples)"]},{"cell_type":"code","execution_count":141,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Diversity sampling prompt:\n","{'role': 'system', 'content': 'You are a semantic parser. Answer as concisely as possible.'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: through which states does the m0 flow'}\n","{'role': 'assistant', 'content': 'answer ( intersection ( state , traverse_1 ( m0 ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: where is m0'}\n","{'role': 'assistant', 'content': 'answer ( loc_1 ( m0 ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: which river traverses most states'}\n","{'role': 'assistant', 'content': 'answer ( most ( river , traverse_2 , state ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: which rivers are in m0'}\n","{'role': 'assistant', 'content': 'answer ( intersection ( river , loc_2 ( m0 ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what states border states that the m0 runs through'}\n","{'role': 'assistant', 'content': 'answer ( intersection ( state , next_to_2 ( intersection ( state , traverse_1 ( m0 ) ) ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what is the highest mountain in m0'}\n","{'role': 'assistant', 'content': 'answer ( highest ( intersection ( mountain , loc_2 ( m0 ) ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: how many inhabitants does m0 have'}\n","{'role': 'assistant', 'content': 'answer ( population_1 ( m0 ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what is the area of m0'}\n","{'role': 'assistant', 'content': 'answer ( area_1 ( m0 ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what is the most populous city'}\n","{'role': 'assistant', 'content': 'answer ( largest_one ( population_1 , city ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what is the highest point in m0 in meters'}\n","{'role': 'assistant', 'content': 'answer ( highest ( intersection ( place , loc_2 ( m0 ) ) ) )'}\n","{'role': 'user', 'content': 'Map the following query into functional query language: what river runs through m0'}\n"]}],"source":["prompt = construct_diversity_prompt(utterance=\"what river runs through m0\",\n","                           training_examples=training_examples,\n","                           embedding_model='all-MiniLM-L6-v2')\n","print(\"Diversity sampling prompt:\")\n","for entry in prompt:\n","    print(entry)"]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 40/40 [13:13<00:00, 19.85s/it]\n"]}],"source":["diversity_predictions = get_predictions(model=\"gpt-3.5-turbo\",\n","                                        evaluation_examples=dev_examples,\n","                                        prompt_creation_function=construct_diversity_prompt,\n","                                        training_examples=training_examples,\n","                                        embedding_model='all-MiniLM-L6-v2')"]},{"cell_type":"code","execution_count":143,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exact match for diversity based prompt: 0.475\n"]}],"source":["# Save the predictions as `diversity_predictions.txt`\n","save_predictions(diversity_predictions, './saves/diversity_predictions.txt')\n","\n","exact_match = evaluate(diversity_predictions, dev_examples)\n","print(f\"Exact match for diversity based prompt: {exact_match}\")"]},{"cell_type":"markdown","metadata":{},"source":["Get the predictions and submit these as `diversity_predictions.txt`, where each line is a prediction for the development set. With the improved selection, we should get least 35\\% exact match on the validation set."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For the report, compare the predictions from the example selection methods using 1) uniform random sampling 2) embedding-based similarity search and 3) coverage based selection. Compare and contrast the errors and submit your analysis as `report.pdf`."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Submission"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Turn in the following files on Gradescope:\n","* hw5.ipynb (this file; please rename to match)\n","* random_predictions.txt\n","* similarity_predictions.txt\n","* diversity_predictions.txt\n","* report.pdf"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.9 ('sp23-hw5')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"vscode":{"interpreter":{"hash":"f09187e429b220fc6015b52991ae62edbd5102028f95369b40ca5737ec09ce99"}}},"nbformat":4,"nbformat_minor":4}
